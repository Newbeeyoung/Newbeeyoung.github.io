[{"title":"ActionRecognition","url":"/2018/09/18/ActionRecognition/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n# Model 1: 3D-CNN\n\n[Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? 2017](https://arxiv.org/abs/1711.09577)\n## Introduction\nThe purpose of this study is to determine whether current video datasets have sufficient data for training very deep CNNs with spatio-temporal 3D kernel.\n\nUCF-101 and HMDB-51 provide videos with size around 10k, ActivityNet has 28k video clips, which are too small to be used for optimizing CNN representation from scratch. Kinetics Dataset has more than 300k video clips (Kinetics 600 has 500k clips now).\n\nThe 3D CNN architectures tested are based on ResNets and their 3D extented versions. The results show that Kinectics dataset can train 3D ResNet-152 from scratch. This is the **first work** to focus on the training of very deep 3D CNNs from scratch for action recognition.\n\n## Experiment Configuration\n\nResNet 18-200, ResNeXt and DenseNet are tested with Kinetics dataset in the study. ResNet 18,34, use basic block (Type A); ResNet 50,101,152,200 use bottleneck block (Type B); Pre-activation ResNet 200 was evaluated as well.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN3.png)\nThose deep CNNs are trained from scratch by Kinetics dataset and fined tune by UCF101, HMDB 51 and ActivityNet to evaluate model's transfer ability.\n\n### Implementation\n\n**Training**:\n\nDataPreprocessing:\n1. A 16-frame clip is generated around selected temporal position\n2. Randomly select a spatial position from the 4 corners and center\n3. Randomly select a spatial scale of sample to perform multi-scale cropping\n4. Spatially resize the sample at 112x112 pixels. The size of each sample is 3x16x112x112\n5. Each sample is horizontally flipped with 50% probability\n6. Subtract the mean values of ActivityNet from the sample for each color channel\n\nHyperparametersï¼š\n1. cross-entropy loss and BP\n2. SGD with weight decay of 0.001 and 0.9 for momentum\n3. When training from scratch, lr starts from 0.1 and divide it by 10 after the validation loss saturates.\n4. When fine tuning, lr starts from 0.001 and has weight decay of 1e-5\n\n**Testing**:\n\n1. Each video is split into non-overlapped 16-frame clips.\n2. Each clip is spatially cropped around center position at scale 1\n3. Average output scores over all the clips of input video\n\n**All video clips in datasets are resized to heights of 240 pixels without changing their aspect ratios.**\n\n## Results and Discussion\n\n### Analysis of training from scratch\n\nTraining from scratch, ResNet-18 overfit both UCF-101, HMDB-51 and ActivityNet. The validation accuracies are 40.1, 16.2 and 26.8% respectively.\n\nKinetics could be used to train ResNet-18 without overfitting. As model depth increased, accuracies improved until reaching the depth of 152.\n\nResNeXt-101 achieved the best accuracies among the architectures testet.\n\nInput of 3x64x112x112 improves the accuracy by 4% on Kinetics dataset. Increase of resolution may also increase the accuracy like 3x64x224x224 in I3D.\n\n### Analysis of fine tuning\n\nIn this section, UCF-101 and HMDB-51 are used to fine tune Kinectics pretrained CNNs. Only conv5_x and FC layers are fine tuned because it achieved the best performance during the preliminary experiments.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN1.png)\n\nBased on results on UCF-101 and HMDB-51, simple 3D architectures pretrained on Kinetics outperform complex 2D architectures.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN2.png)\n\n## Conclusion\n\n1. ResNet-18 training resulted in  significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics.\n2. Kinetics dataset has sufficient data for training deep 3D CNNs and enabling training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet.\n3. Kinetics pretrained simple 3D architectures outperforms complex 2D architectures on UCF-101 and HMDB-51, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51.\n\n\n# Model 2: Two-Stream I3D\n\n[Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset 2017](https://arxiv.org/abs/1705.07750)\n\nTwo-Stream Inflated 3D ConvNets(I3D) builds upon Inception V1, but inflates their filters and pooling kernels into 3D, leading to very deep, naturally spatio-temporal classifier.\n\n## Architecture\n\n### Inflating 2D ConvNets into 3D\n\nSimply inflating all the filters and pooling kernels in 2D architecture - endowing them with an additional temporal dimension. Filters are typically square and we just make them cubic - NxN to NxNxN\n\n### Bootstrapping 3D filters from 2D filters\n\nTo bootstrap parameters from the pretrained ImageNet models, it repeats the weights of 2D filters N times along the time dimension and rescaling them by dividing by N. \n\n### Pacing receptive field growth in space, time and network depth\n\nA symmetric receptive field is not necessarily optimal when considering time, it should depend on frame rate and image dimensions. If it grows too quickly in time relative to space, it may conflate edges from different objects breaking early feature detection, while if it grows too slowly, it may not capture scene dynamics well.\n\n64-frame snippets are used in training, the whole video is used and predictions are averaged in testing.\n\nThe overall architecture is shown below:\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537175423/blog/4.%20Action%20Recognition/I3D1.png)\n\n### Two 3D Streams\n\nIn experiments, it is found that two-stream configuration-with one I3D network trained on RGB inputs, and another on flow inputs which carry optimized, smooth flow information is valuable. These 2 networks are **trained separately** and predictions are averaged in test time.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537175947/blog/4.%20Action%20Recognition/I3D2.png)\n\n\n## Implementation\n**Traning:**\n\nData Preprocessing:\n1. video is sampled to 25 frames per second \n2. Resizing the smaller video side to 256 pixels, then randomly cropping 224x224 patch\n3. For shorter videos, we looped the video as many times as necessary to satisfy model's input interface\n4. Random left-right flipping applied\n\nHyperparameter:\n\n1. For all architectures, each convolutional layer follow by BN layer and ReLU activation function, except for the last convolutional layer which produce the class scores for each network.\n2. Standard SGD with momentun set to 0.9.\n3. 3D ConvNets utilize 64 GPUs with synchronous parallelization.\n4. Trained model on Kinetics fro 110k steps with 10x reduction of learning rate when val loss saturated.\n5. Implemented in Tensorflow\n\n**Test:**\nThe models are applied convolutionally over the whole video taking 224x224 center crops.\n\n**Optical Flow is computed with TV-L1 algorithm.**\n\n## Experiment and Discussion\n\n1. I3D outperformed all previous model, with either RGB, flow, or RGB+flow modalities.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537178242/blog/4.%20Action%20Recognition/I3D3.png)\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537178242/blog/4.%20Action%20Recognition/I3D4.png)\n2. ImageNet pre-training can extend to 3D ConvNets, improves accuracy.\n3. It seems plausible that RGB stream has more discriminative information - we often struggled with eyes to discern actions from flow alone in Kinetics. **There may be opportunities fro future research on integrating some form of motion stabilization into these architectures.**\n\n4. The study demonstrates transfer learning from one dataset(Kinetics) to anohter dataset(UCF-101/HMDB-51) for similar task. **However, it still remains to be seen if there is a benefit in using Kinetics pre-training for other video task such as semantic video segmentation, video object detection, or optical flow computation.**\n5. Action tubes or attention mechanism could be employed in the future.\n\n# Model 3: CNN+LSTM\n\n[Long-term Recurrent Convolutional Networks for Visual Recognition and Description 2014](https://arxiv.org/abs/1411.4389)\n\n## Introduction\n\nThis study instantiates proposed architecture in 3 experimental settings.\n1. Directly connect CNN model to deep LSTM network, to train video recognition models, which improves on the order of 4% on conventional benchmark\n2. Use CNN to encdoe a state vector, an LSTM to decode the vector into an natural language string, to train an end-to-end image to sentence mapping.\n3. Use conventional computer vision method to predict higher-level discriminative labels, enables LSTM decoders to generate language string\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM1.png)\n\n## Architecture\n\nThe LRCN model works by passing each visual input $v_{t}$ (an image in isolation, or a frame from a video) through a feature transformation $\\phi_{V}(v_{t})$ parametrized by $V$ to produce a fixed-length vector representation $\\phi_{t}\\in R^d$. Having computed the feature space representation of the visual input sequence $<\\phi_{1},\\phi_{2},...,\\phi_{T}>$, the sequence model then takes over.\n\nThe final step in predicting a distribution $P(y_{t})$ is to take a softmax over the output $z_{t}$ of the sequence model.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM2.png)\nLRCN models is trained using SGD with momentum, with backpropagation to compute the gradient $\\nabla L(V,W)$ of the objective $L$ with respect to all parameters $(V,W)$. $W$ is parameter of LSTM, $V$ is parameter of CNN.\n\n## Action Recognition\n\nT individual frames are inputs into T convolutional networks which are then connected to a single-layer LSTM with 2565 hidden units. The study analyzes clips of 16 frames.\n\nIt explores 2 varaints of LRCN: LSTM is placed after CNN fc6 and CNN fc7. \n\n### Implementation\nIn training, video clip of 16 frames is fed into model. LRCN predicts the video class at each time step and then it averages these predicitons for final classification\n\nIn test, they extract 16 frame clips with a stride of 8 frames from each video and average across clips.\n\nBoth RGB and optical flow are used as input.\n\nCNN base of the LRCN is a hybrid of the Caffe reference model, a mior variant of AlexNet. The net is pre-trained on the 1.2 M image ILSVRC-2012  classification training subset of the ImageNet dataset.\n\n### Result\n\nRGB and flow networks can be combined by computing a weighted averagew fo network scores, which leads to better accuracy.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM3.png)\n\n# Dataset\n\n## Kinetics-600\n\nhttps://deepmind.com/research/open-source/open-source-datasets/kinetics/\n\n[The Kinetics Human Action Video Dataset](https://arxiv.org/abs/1705.06950)\n\nSize: around 500k\n\nClass: 600\n\nYear: 2018\n\nSource: \n1. The clips are from YouTube video, last 10s, and have a variable resolution and frame rate\n2. For an action class, all clips are from different YouTube videos\n\nNote: There is a standard test set (label released), and also a held-out test set(label not released, used for AcitivityNet Challenge). It encourage researchers to report results on the standard test set. \n## UCF-101\nhttp://crcv.ucf.edu/data/UCF101.php\n\n[UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild](http://crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)\n\n\nSize: 13k\n\nLength: 27h\n\nClass: 101\n\nYear: 2011\n\nSource: realistc user-uploaded videos containing camera motion and cluttered background\n\n## HMDB-51\n\nhttp://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\n\n[HMDB: A Large Video Database for Human Motion Recognition. ICCV, 2011](http://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)\n\nSize: 7000 \n\nClass: 51\n\nYear: 2011\n\nSource: Range from digitized movies to YouTube\n\n\n\n\n\n\n\n\n\n","tags":["Action Recognition"]},{"title":"Summary from 23 Jul-10 Sep","url":"/2018/09/11/Summary-August-2018(1)/","content":"![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536636693/blog/2.%20Summary%20of%20August%202018/merge_from_ofoct.jpg)","tags":["Essay"]},{"title":"Spatial Transformer Network - Notes","url":"/2018/09/06/Notes/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n**In many computer vision tasks, distortion and transformation of image is highly needed. For example, in Cloth Visual Try on, which is the area i am interested in, cloth has to be distorted before putting on real-world human. Spatial Transfromer Network has shown impressive results in terms of image transformation and attention. It is worth reading this paper slowly and carefully.**\n\n***\n# Abstract\nSpatial Transformer Network explicitly allows the spatial manipulation of data within the network.\n\nit can be inserted into existing cnn to spatially transform feature maps and no extra tranining supervision or modification needed.\n\n# Introduction\nAction of spatial transformer is conditioned on individual data samples, with behaviour learnt during training.\n\nNot only select regions of an image that are most relevant, but aslo to transform those regions to canonical, expected pose.\n\nIt is trained with standard BP\n\n**application**\n1. image classification\n2. co-localisation\n3. **spatial attention**\n\na generalisation of differentiable attention to any transformation\n\n# Spatial Transformer Network\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/Spatial_transformer_network.png)\n\n3 parts:\n\n- **localisation network**: take input feature map and outputs parameters of the spatial transformation that should be applied to feature map--- gives transformation conditional on the input\n- **grid generator**: parameters form part 1 are used to create a sampling grid, wihch is a set of points where the input map should be sampled to produce the transformed output\n- **sampler**: feature map and sampling grid as input to sampler, producing output map sampled from the input at the grid points\n\n## Localisation Network\n\n\n$$\n\\theta=f_{loc}(U)\n$$\n\n$\\theta$ is output, $U$ is input, $f_{loc}$ is network function, it can take any form including FC network, CNN, but should include a final regression layer to produce transformation parameters $\\theta$\n\n## Parameterised Sampling Grid\nTo perform warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at particular location in the input feature map.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/sampling_grid.png)\nAssume $T_{\\theta}$ is a 2D affine transformation $A_{\\theta}$.\n\n$$\n\\left(\\begin{array}{cc}x_{i}^s \\\\ y_{i}^s\\end{array}\\right)=A_{\\theta}\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)=\\left[\\begin{array}{cc}\\theta_{11} &\\theta_{12} &\\theta_{13} \\\\ \\theta_{21} &\\theta_{22} &\\theta_{23}\\end{array}\\right]\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)\n$$\n\n$(x_{i}^t,y_{i}^t)$ are the target coordinates of the regular grid in the output feature map.\n\n$(x_{i}^s,y_{i}^s)$ are the source coordinates in the input feature map that define the sample points\n\n$A_{\\theta}$ is the affine transformation matrix\n\nOf course, $ T_{\\theta}$ or $A_{\\theta}$ can have any parameterised form, provided that it is deffirentiable with respect to the parameters---for BP\n\n**Thin plate spline transformation(TPS) is the most powerful transformation in experiments.**\n\n## Differentiable Image Sampling\n\nTo perform a spatial transformation of the input feature map, a sampler must take the set of sampling points $T_{\\theta}(G)$, along the the  input feature map $U$ and produce the smapled output feature map $V$\n\nEach $(x_{i}^s,y_{i}^d)$ coordinate in $T_{\\theta}(G)$ defines the spatial location in the input where a sampling kernel(such as bilinear) is applied to get the value at the particular pixel in the output $V$\n\n# Experiment\nExperiments are conducted on Distorted MNIST, Street View House Number for number recognition and CUB-200-2011 birds dataset by automatically discovering object parts and learning to attend to them.\n\n## Distorted MNIST\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/MNIST.png)\n\nSpatial transformer network acts on the input before the classification network, which are FCN and CNN, all STN use bilinear sampling, but use different transformation functions: affine, projective, 16 point thin plate spline (TPS)\n\n# Street View House Number\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228784/blog/1.%20Spatial%20Transformer%20Network/street_view_number.png)\nSpatial transformer network acts on the input before baseline CNN, and define another extension where before each of the first 4 convolutional layer of the baseline CNN. Regression layers of spatial transformer are initialised to predict the identity transformation. Affine transformation and bilinear sampling kernel are used\n\n# Fine-Grained Classification\n\nST-CNN is able to discover and learn part detectores in a data-driven manner without any additional supervision\n\n# Conclusion\n\nAccuracy improved in many tasks using spatial transformer network, it would be useful for tasks requiring the disentangling of object reference frames.\n\n\n\n","tags":["Transformation"]},{"title":"Hello World","url":"/2018/09/06/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]