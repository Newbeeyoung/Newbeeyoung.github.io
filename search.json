[{"title":"Dynamic Programming Summary","url":"/2018/10/08/dynamic-programming/","content":"\n**I have spent dozens of hours in last month on Dynamic Programming, it is time to go through all the questions and make a conclusion. I believe a good summary would be as important as finishing 50 questions.**\n\n# Basic Concept\n\nIn my view, **dynamic programming** consists of two key concepts: \n1. State\n2. Recursive or iterative relation between state(Transition Function)\n\nState, some time could be regarded as storage, which leads to **Memoization** and **Tabulation** in dynamic programming. Current state could be generated by previous state using transition function.\n\nThe key of solving dynamic programming problem is always finding the **relation**. Of course, it requires a lot of practice. \n\n# Basic Method\n\nMost of the blogs and tutorials in dynamic progamming would introduce 2 major methods:\n\n1. Memoization - Top down\n2. Tabulation - Bottom up\n\nHere is a great article explaining these 2 methods:\nhttps://www.geeksforgeeks.org/tabulation-vs-memoizatation/. In simple words, both of them have tables storing state. Top down with recursive method, will use table entry directly if it is not null, otherwise it will fill the table entry, make it available in the next recursion. Bottom up with iterative loop will fill table entry from left top to right bottom. The latter states depends on previous states.\n\n# Summary of Leetcode questions\n\nHaving finished most of **EASY** and **MEDIUM** questions in dynamic programming in Leetcode, i summarized all the questions to find some hidden regularities.\n\n## Type 1 - Bottom up:Time Complexity O(N) Space Complexity O(1)\n\nThe optimal solution for ith item is only determined by most recent states, so not all previous states need to be stored. For example, in \"House Robber\" problem: \n```\ndp[i]=max(dp[i-2],dp[i-3])+house[i]\n```\nIn this case, dp[i] must contains house[i], which means robber must rob house[i],so we need to compare dp[i] with previous dp to find optimal solution. Sometimes there 2 states for dp, like \"Best Time to Buy Stock\" problem:\n```\ndp[i][0]=max(dp[i-1][1]+price[i],dp[i-1][0])\ndp[i][1]=max(dp[i-1][0]-price[i],dp[i-1][1])\n```\n\n## Type 2 - Bottom up: Time Complexity O(N) Space Complexity O(N)\n\nTo find the optimal solution of ith item, we need to find the optimal item which fufill requirements in range [0,i]. It requires all the previous states stored. For example, in \"Perfect Square\",\n\n```\nfor i in range(n):\n    min_=sys.maxint\n    for j in range(i):\n        min_=min(dp[j],min_)\n    dp[i]=min_+1\n```\n\n## Type 3 - Top Down: Time Complexity O(N) Space Complexity O(N)\n\nTop-Down is memoiation. For example, \"Cheapest flights with K stop\":\n\n```\n    def bfs(n,flightlist,dst,s,k,mem):\n            \n            if k<-1:\n                return sys.maxint\n            \n            if s==dst:\n                return 0\n            else:\n                if mem[s][k]!=0:\n                    return mem[s][k]\n                \n                tmp=sys.maxint\n                for i in range(n):\n                    if flightlist[s][i]!=0:\n                        tmp=min(bfs(n,flightlist,dst,i,k-1,mem)+flightlist[s][i],tmp)\n                mem[s][k]=tmp\n                return tmp\n```\n\n## Bottom up\n\n|No | Question |State |State Relation|\n|---|---|---|---|\n63 | Unique Path|number of unique path dp[i][j]|dp[i][j]=dp[i-1][j]+dp[i][j-1]\n64 | Minimum path sum | min path sum dp[i][j]|dp[i][j]=min(dp[i-1][j],dp[i][j-1])+grid[i][j]\n120| Triangle |min triangle path sum dp[i][j] |dp[i][j]=min(dp[i-1][j],dp[i-1][j-1])+triangle[i][j]\n152| Maximum product subarray | max product dp[i] | dp[i]=dp[i-1]*nums[i] (actually more complex due to positive and negative products)\n**198,213**|**House Robber I,II** | max amount of robbery dp[i] | dp[i]=max(dp[i-2],dp[i-3])+house[i] (Q II introduces circle, more condition to consider)\n**279**| **Perfect Squares** |least number of perfect squares dp[n]|Typical: for i in range(n):for j in range(i): dp[n]=min(dp[j])+1. Current optimal case comes from the optimal case within all the previous states. Space complexity is o(N)\n300| Longest Increasing Subsequence | longest increasing subsequence dp[n] | Simlilar to 279. for i in range(n):for j in range(i): if nums[i]>nums[j]:dp[i]=max(dp[j])+1\n**121** |**Best Time to buy and sell stock SERIES**|max value when holding and not holding stock after k transaction at nth day dp[n][k][1],dp[n][k][0] |dp[n][k][1]=max(dp[n-1][k][1],dp[n-1][k-1][0]-price[n]); dp[n][k][0]=max(dp[n-1][k][0],dp[n-1][k][1]+price[n]) only buy counted as one transaction in this assumption\n322| Coin change | min number of coin add up to amount n dp[n]|Similar to 279,300\n338| Counting Bits | number of 1's in binary form dp[n] | dp[n]=dp[n/2]+n%2 Found by sampling\n343| Integer Break | break integers to at least 2 integers and achieve max product dp[n] | Similar to 279,300\n211| Maximal square|max square from (0,0) to (i,j) dp[i][j] |dp[i][j]=min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+1\n**53**| **Maximual Subarray** |max subarray which must contains nums[n]: dp[n]| dp[n]=dp[n-1]+nums[n] if dp[n-1]>0 else dp[n]=nums[n] return max(dp)\n70| Climbing Stairs | distinct ways climbing to nth step dp[n]|dp[n]=dp[n-1]+dp[n-2]\n368| Largest Divisible Subset | largest divisible subset which must contains nums[n]: dp[n] |similar to 53\n377| Combination Sum IV | possible combinations dp[n]|dp[n]+=dp[n-item] for item in nums\n**416** | **Partition Equal Subset Sum (0/1 knapsack problem)**| specific sum j could be obtained from first i numbers: dp[i][j]| dp[i][j]=dp[i-1][j] or dp[i-1][j-nums[i]]\n**413** |**Arithmetic Slices** | number of slices end at nums[i]: dp[i]|if nums[i]-nums[i-1]==nums[i-1]-nums[i-2]: dp[i]=dp[i-1]+1\n**516**|**Longest Palindromic Subsequence**|longest palidromic subsequence length from i to j: dp[i][j]| if s[i]==s[j]:dp[i][j]=2+dp[i+1][j-1] if j-1>i+1 else 2, else dp[i][j]=max(dp[i+1)[j],dp[i][j-1]\n650|Two Key keyboard|min step needed to form n 'A': dp[n]| for i in range(n): for j in reversed(range(i-1)): if i%j==0:dp[i]=dp[j]+i/j\n673|Number of Longest Increasing Subsequence|longest increasing subsequence end with nums[n]|Upgrade version of 300 \n**718**|**Maximum Length of Repeated Subarray**|length of longest substring that end with A[i] and B[j]:dp[i][j]|if A[i]==B[j]:dp[i][j]=dp[i-1][j-1]+1\n712|Minimum ASCII Delete Sum for Two Strings|min sum of delete strings end with s1[i],s2[j]:dp[i][j]|similar to 718\n746|Min Cost Climbing Stairs|min cost which must contains cost[n]:dp[n]|dp[n]=min(dp[n-1],dp[n-2])+cost[n]\n**790**|**Domino and T Domino Tiling**| number of ways to form rectangle with lenght n:dp[n]|dp[n]=2*dp[n-1]+dp[n-3] (Quite hard to find)\n813|Largest Sum of Average| Largest sum in first i elements with k partitions: dp[i][k]|for k in range(K):for j in range(i):dp[i][k]=max(dp[i][k],sum(List[j:i])/(i-j)\n873|Length of longest Fibonacci Sequence| lenght of longest Fibonacci sequence from List[j] to List[i]: dp[j][i]|for i in range(len(List):for j in range(i):if List[i]-List[j] exist:dp[j][i]=dp[List[i]-List[j]][i]+1 ,result=max(result,max[j][i] \n\n\n\n## Top down\n\nNo|Question|Memory|Transition Function\n---|---|---|---\n139|Word Break| Substrings which cannot be broken into words |for word in words: if s[start:start+len(word)]==word: if recur():return true\n494|Target Sum|number of ways that S==sum(i numbers): mem[i][S]|count(i+1,s+nums[i])+count(i+1,s-nums[i])\n576|Out of Boundary Paths|number of paths at (i,j) and N steps left| path[i][j][N]=findpath(i,j+1,N-1)+findpath(i+1,j,N-1)+findpath(i-1,j)+findpath(i,j-1)\n646 |Maximum Length of Pair Chain||longest chain end with nums[n]:dp[n]|similar to 279\n688|Knight Probability in Chessboard|number of path that knight will go out of chessboard when k steps left|for i in range(8):count+=count(k-1,row+pair[i][0],column+pair[i][1])\n**787**|**Cheapest Flights with K Stops**| Min cost path which must contain flight i at k stop: mem[i][k]|for i in range(n):mem[i][k]=min(mem[s][k-1]+flight[i][k])\n\n\n## Special Case\n\nNo|Question | State | State Relation\n---|---|---|---\n264| Ugly Number II | nth ugly number dp[n] |Update mutiplier of 2,3,5 independently to find min ugly number dp[n] which is just larger than dp[n-1]\n304| Range Sum Query 2D -Immutable | sum of elements from (0,0) to (n,n) Sum[n][n] | Area = Sum[i2][j2]-Sum[i2][j1]-Sum[i1][j2]+Sum[i1][j1]. Immutable means the function would be callled many times, we need initiate states in __init__function.\n376| Wiggle Subsequence ||Greedy algorithm: only need to find number of max and min peak in sequence\n392| Is Subsequence |index of s | if s[index]==t[i]: index+=1 When index>len(s), the whole substring is found\n467| Unique Substrings in Wraparound String |Keep tracking longest string end with 'a-z': dict{'a-z':count} |Sum+=count(a-z), upgrade version of 413. The aggregation of number of substring follows the same rule: count(x)=length of longest string end with x \n647|Palindromic substrings| **Manacher's Algorithm**/Expand from center\n801|Minimun Swaps To Make Increasing Sequence|Wether swap A[i] and B[i] only depends on A[i-1] and B[i-1], so two dp variable should be updated. Best condition when A[i] and B[i] swap and best condition when A[i] and B[i] not swap.\n838|Pushing Dominos|final result and status of previous and current dominoes |\n","tags":["Leetcode"]},{"title":"Face Swap Summary","url":"/2018/09/30/faceswap/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n\n# Face Swap Method 1\n\nFace swapping with Python, dlib, OpenCV \n\nhttps://github.com/matthewearl/faceswap\n\nFace swapping with Python dlib, Skimage(much easier in alignment)\n\nhttps://github.com/marsbroshok/face-replace\n\nFace Swapping with C++, dlib, histogram\n\n## Step\n1. Detecting facial landmarks\n2. Rotating, scaling, translating the second image to fit over the first (cv::getAffineTransoform or using SVD)\n3. Adjusting the color balance in the second image to match that of the first(another uses histogram matching)\n4. Blending feature from the second image on top of the first (A mask is used to select which parts of image2 and which parts of image1 should be shown in the final image)\n\n# Face Swap Method 2\n\nFace replace with Kinect, OpenCV, OpenGL\n\nhttps://github.com/jorticus/face-replace\n\n## Step\n1. Capture RGB+Depth video frame by Kinect\n2. Detect head pose and face features using Kinect SDK\n3. Deform the Candide-3 mesh to the given head pose and face features\n4. Process the RGB+Depth frames using OpenCV\n5. Draw the RGB video frame\n6. Draw the texture-mapped candide-3 model in OpenGL using a custom blend shader\n\n\n# Face Swap Method 3\n\nFace swapping with Python, OpenCV, dlib, pygame, PyOpenGL\n\nhttps://github.com/MarekKowalski/FaceSwap\n\n1. Detect face and landmarks\n2. Fit 3D model to landmarks\n3. Render 3D model using pygame with texture obtained during initialization\n4. Image of rendered model is blended with image obtained from camera using feathering(alpha blending) and simple color correction\n\n**Solved** Problems:\n1. Facial geometry of people varies quite a bit\n2. The lighting on the face combined with the tone of the skin can make the image look very different\n\n**Unsolved** Problems:\n1. The pose of the face can vary significantly\n2. The texture of the skin can vary from smooth to almost leathery\n\n# Face Swap Method 4 (similar to Method 1)\n\nOpenCV own lib\n\nhttps://github.com/spmallick/learnopencv/tree/master/FaceSwap\n\n**https://www.learnopencv.com/face-swap-using-opencv-c-python**\n\nhttps://github.com/t0nyren/piecewiseAffine\nhttps://github.com/trishume/faceHack\n\n1. Facial landmarks detection\n2. find convex hull\n3. delaunay triangulation\n4. affine wap triangles\n5. seamless cloning (自动合成)\n\n# Face Swap Method 5\n\nhttps://github.com/YuvalNirkin/face_swap\n\n\"On Face Segmentation, Face Swapping, and Face Perception\"\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537439562/blog/3.%20Face%20Swap/FaceSwap1.png)\n\n1. Fitting 3D face shapes\n2. Deep face segmentation\n3. Face Swapping and blending\n\n# Face Swap Method 6\n\n\"Transfiguring Portrait SIGGRAPH 2016\"\nhttps://homes.cs.washington.edu/~kemelmi/Transfiguring_Portraits_Kemelmacher_SIGGRAPH2016.pdf\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537948435/blog/3.%20Face%20Swap/FaceSwap2.png)\n\n## Face Processing\n1. Dectect Face and face landmarks\n2. Align and warp face to 3D model, estimate 3D pose\n3. Estimate features per photo: age, gender, HoG (histogram of gradient), face recognition features by VGG. \n\n## Similar set estimation\n\nPhotos of very similar face shapes are more amenable to effective composition. Target images are searched from online and typically include 1000 photos.\n\n1. Rank target photos based on their similarity to input photo. The similarity is computed as L2 norm between VGG face recognition features.\n2. Rerank selected photos in stage 1 by pose, age, HoG features:\n\n$$\nD(s,t)=||P_{s}-P_{t}||^2+||Age_{s}-Age_{t}||^2+X^2(H_{s},H_{t})\n$$\n\n## Systhesis\n\n1. Using Skin and hair algorithm to estimate mask\n2. Calculate an edge map from each mask and compute L2 norm between two masks. Pairs with smaller difference are ranked higher\n3. Align two photos using landm points, blend two photos given two mask by (Levin, 2004 Seamless image stitching in the gradient domain). \n\n\n# Real-time swapping \n\nDeepfake\n\nhttps://github.com/shaoanlu/faceswap-GAN\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1538318380/blog/3.%20Face%20Swap/deepfake.jpg)","tags":["Summary"]},{"title":"Action Recognition Model and Dataset Summary","url":"/2018/09/18/ActionRecognition/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n# Model 1: 3D-CNN\n\n[Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? 2017](https://arxiv.org/abs/1711.09577)\n## Introduction\nThe purpose of this study is to determine whether current video datasets have sufficient data for training very deep CNNs with spatio-temporal 3D kernel.\n\nUCF-101 and HMDB-51 provide videos with size around 10k, ActivityNet has 28k video clips, which are too small to be used for optimizing CNN representation from scratch. Kinetics Dataset has more than 300k video clips (Kinetics 600 has 500k clips now).\n\nThe 3D CNN architectures tested are based on ResNets and their 3D extented versions. The results show that Kinectics dataset can train 3D ResNet-152 from scratch. This is the **first work** to focus on the training of very deep 3D CNNs from scratch for action recognition.\n\n## Experiment Configuration\n\nResNet 18-200, ResNeXt and DenseNet are tested with Kinetics dataset in the study. ResNet 18,34, use basic block (Type A); ResNet 50,101,152,200 use bottleneck block (Type B); Pre-activation ResNet 200 was evaluated as well.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN3.png)\nThose deep CNNs are trained from scratch by Kinetics dataset and fined tune by UCF101, HMDB 51 and ActivityNet to evaluate model's transfer ability.\n\n### Implementation\n\n**Training**:\n\nDataPreprocessing:\n1. A 16-frame clip is generated around selected temporal position\n2. Randomly select a spatial position from the 4 corners and center\n3. Randomly select a spatial scale of sample to perform multi-scale cropping\n4. Spatially resize the sample at 112x112 pixels. The size of each sample is 3x16x112x112\n5. Each sample is horizontally flipped with 50% probability\n6. Subtract the mean values of ActivityNet from the sample for each color channel\n\nHyperparameters：\n1. cross-entropy loss and BP\n2. SGD with weight decay of 0.001 and 0.9 for momentum\n3. When training from scratch, lr starts from 0.1 and divide it by 10 after the validation loss saturates.\n4. When fine tuning, lr starts from 0.001 and has weight decay of 1e-5\n\n**Testing**:\n\n1. Each video is split into non-overlapped 16-frame clips.\n2. Each clip is spatially cropped around center position at scale 1\n3. Average output scores over all the clips of input video\n\n**All video clips in datasets are resized to heights of 240 pixels without changing their aspect ratios.**\n\n## Results and Discussion\n\n### Analysis of training from scratch\n\nTraining from scratch, ResNet-18 overfit both UCF-101, HMDB-51 and ActivityNet. The validation accuracies are 40.1, 16.2 and 26.8% respectively.\n\nKinetics could be used to train ResNet-18 without overfitting. As model depth increased, accuracies improved until reaching the depth of 152.\n\nResNeXt-101 achieved the best accuracies among the architectures testet.\n\nInput of 3x64x112x112 improves the accuracy by 4% on Kinetics dataset. Increase of resolution may also increase the accuracy like 3x64x224x224 in I3D.\n\n### Analysis of fine tuning\n\nIn this section, UCF-101 and HMDB-51 are used to fine tune Kinectics pretrained CNNs. Only conv5_x and FC layers are fine tuned because it achieved the best performance during the preliminary experiments.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN1.png)\n\nBased on results on UCF-101 and HMDB-51, simple 3D architectures pretrained on Kinetics outperform complex 2D architectures.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN2.png)\n\n## Conclusion\n\n1. ResNet-18 training resulted in  significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics.\n2. Kinetics dataset has sufficient data for training deep 3D CNNs and enabling training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet.\n3. Kinetics pretrained simple 3D architectures outperforms complex 2D architectures on UCF-101 and HMDB-51, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51.\n\n\n# Model 2: Two-Stream I3D\n\n[Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset 2017](https://arxiv.org/abs/1705.07750)\n\nTwo-Stream Inflated 3D ConvNets(I3D) builds upon Inception V1, but inflates their filters and pooling kernels into 3D, leading to very deep, naturally spatio-temporal classifier.\n\n## Architecture\n\n### Inflating 2D ConvNets into 3D\n\nSimply inflating all the filters and pooling kernels in 2D architecture - endowing them with an additional temporal dimension. Filters are typically square and we just make them cubic - NxN to NxNxN\n\n### Bootstrapping 3D filters from 2D filters\n\nTo bootstrap parameters from the pretrained ImageNet models, it repeats the weights of 2D filters N times along the time dimension and rescaling them by dividing by N. \n\n### Pacing receptive field growth in space, time and network depth\n\nA symmetric receptive field is not necessarily optimal when considering time, it should depend on frame rate and image dimensions. If it grows too quickly in time relative to space, it may conflate edges from different objects breaking early feature detection, while if it grows too slowly, it may not capture scene dynamics well.\n\n64-frame snippets are used in training, the whole video is used and predictions are averaged in testing.\n\nThe overall architecture is shown below:\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537175423/blog/4.%20Action%20Recognition/I3D1.png)\n\n### Two 3D Streams\n\nIn experiments, it is found that two-stream configuration-with one I3D network trained on RGB inputs, and another on flow inputs which carry optimized, smooth flow information is valuable. These 2 networks are **trained separately** and predictions are averaged in test time.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537175947/blog/4.%20Action%20Recognition/I3D2.png)\n\n\n## Implementation\n**Traning:**\n\nData Preprocessing:\n1. video is sampled to 25 frames per second \n2. Resizing the smaller video side to 256 pixels, then randomly cropping 224x224 patch\n3. For shorter videos, we looped the video as many times as necessary to satisfy model's input interface\n4. Random left-right flipping applied\n\nHyperparameter:\n\n1. For all architectures, each convolutional layer follow by BN layer and ReLU activation function, except for the last convolutional layer which produce the class scores for each network.\n2. Standard SGD with momentun set to 0.9.\n3. 3D ConvNets utilize 64 GPUs with synchronous parallelization.\n4. Trained model on Kinetics fro 110k steps with 10x reduction of learning rate when val loss saturated.\n5. Implemented in Tensorflow\n\n**Test:**\nThe models are applied convolutionally over the whole video taking 224x224 center crops.\n\n**Optical Flow is computed with TV-L1 algorithm.**\n\n## Experiment and Discussion\n\n1. I3D outperformed all previous model, with either RGB, flow, or RGB+flow modalities.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537178242/blog/4.%20Action%20Recognition/I3D3.png)\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537178242/blog/4.%20Action%20Recognition/I3D4.png)\n2. ImageNet pre-training can extend to 3D ConvNets, improves accuracy.\n3. It seems plausible that RGB stream has more discriminative information - we often struggled with eyes to discern actions from flow alone in Kinetics. **There may be opportunities fro future research on integrating some form of motion stabilization into these architectures.**\n\n4. The study demonstrates transfer learning from one dataset(Kinetics) to anohter dataset(UCF-101/HMDB-51) for similar task. **However, it still remains to be seen if there is a benefit in using Kinetics pre-training for other video task such as semantic video segmentation, video object detection, or optical flow computation.**\n5. Action tubes or attention mechanism could be employed in the future.\n\n# Model 3: CNN+LSTM\n\n[Long-term Recurrent Convolutional Networks for Visual Recognition and Description 2014](https://arxiv.org/abs/1411.4389)\n\n## Introduction\n\nThis study instantiates proposed architecture in 3 experimental settings.\n1. Directly connect CNN model to deep LSTM network, to train video recognition models, which improves on the order of 4% on conventional benchmark\n2. Use CNN to encdoe a state vector, an LSTM to decode the vector into an natural language string, to train an end-to-end image to sentence mapping.\n3. Use conventional computer vision method to predict higher-level discriminative labels, enables LSTM decoders to generate language string\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537262401/blog/4.%20Action%20Recognition/LSTM_modified.jpg)\n\n## Architecture\n\nThe LRCN model works by passing each visual input $v_{t}$ (an image in isolation, or a frame from a video) through a feature transformation $\\phi_{V}(v_{t})$ parametrized by $V$ to produce a fixed-length vector representation $\\phi_{t}\\in R^d$. Having computed the feature space representation of the visual input sequence $<\\phi_{1},\\phi_{2},...,\\phi_{T}>$, the sequence model then takes over.\n\nThe final step in predicting a distribution $P(y_{t})$ is to take a softmax over the output $z_{t}$ of the sequence model.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM2.png)\nLRCN models is trained using SGD with momentum, with backpropagation to compute the gradient $\\nabla L(V,W)$ of the objective $L$ with respect to all parameters $(V,W)$. $W$ is parameter of LSTM, $V$ is parameter of CNN.\n\n## Action Recognition\n\nT individual frames are inputs into T convolutional networks which are then connected to a single-layer LSTM with 2565 hidden units. The study analyzes clips of 16 frames.\n\nIt explores 2 varaints of LRCN: LSTM is placed after CNN fc6 and CNN fc7. \n\n### Implementation\nIn training, video clip of 16 frames is fed into model. LRCN predicts the video class at each time step and then it averages these predicitons for final classification\n\nIn test, they extract 16 frame clips with a stride of 8 frames from each video and average across clips.\n\nBoth RGB and optical flow are used as input.\n\nCNN base of the LRCN is a hybrid of the Caffe reference model, a mior variant of AlexNet. The net is pre-trained on the 1.2 M image ILSVRC-2012  classification training subset of the ImageNet dataset.\n\n### Result\n\nRGB and flow networks can be combined by computing a weighted averagew fo network scores, which leads to better accuracy.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM3.png)\n\n# Dataset\n\n## Kinetics-600\n\nhttps://deepmind.com/research/open-source/open-source-datasets/kinetics/\n\n[The Kinetics Human Action Video Dataset](https://arxiv.org/abs/1705.06950)\n\nSize: around 500k\n\nClass: 600\n\nYear: 2018\n\nSource: \n1. The clips are from YouTube video, last 10s, and have a variable resolution and frame rate\n2. For an action class, all clips are from different YouTube videos\n\nNote: There is a standard test set (label released), and also a held-out test set(label not released, used for AcitivityNet Challenge). It encourage researchers to report results on the standard test set. \n## UCF-101\nhttp://crcv.ucf.edu/data/UCF101.php\n\n[UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild](http://crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)\n\n\nSize: 13k\n\nLength: 27h\n\nClass: 101\n\nYear: 2011\n\nSource: realistc user-uploaded videos containing camera motion and cluttered background\n\n## HMDB-51\n\nhttp://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\n\n[HMDB: A Large Video Database for Human Motion Recognition. ICCV, 2011](http://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)\n\nSize: 7000 \n\nClass: 51\n\nYear: 2011\n\nSource: Range from digitized movies to YouTube\n\n\n\n\n\n\n\n\n\n","tags":["Action Recognition"]},{"title":"Summary from 23 Jul-10 Sep","url":"/2018/09/11/Summary-August-2018(1)/","content":"![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536636693/blog/2.%20Summary%20of%20August%202018/merge_from_ofoct.jpg)","tags":["Essay"]},{"title":"Spatial Transformer Network - Notes","url":"/2018/09/06/Notes/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n**In many computer vision tasks, distortion and transformation of image is highly needed. For example, in Cloth Visual Try on, which is the area i am interested in, cloth has to be distorted before putting on real-world human. Spatial Transfromer Network has shown impressive results in terms of image transformation and attention. It is worth reading this paper slowly and carefully.**\n\n***\n# Abstract\nSpatial Transformer Network explicitly allows the spatial manipulation of data within the network.\n\nit can be inserted into existing cnn to spatially transform feature maps and no extra tranining supervision or modification needed.\n\n# Introduction\nAction of spatial transformer is conditioned on individual data samples, with behaviour learnt during training.\n\nNot only select regions of an image that are most relevant, but aslo to transform those regions to canonical, expected pose.\n\nIt is trained with standard BP\n\n**application**\n1. image classification\n2. co-localisation\n3. **spatial attention**\n\na generalisation of differentiable attention to any transformation\n\n# Spatial Transformer Network\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/Spatial_transformer_network.png)\n\n3 parts:\n\n- **localisation network**: take input feature map and outputs parameters of the spatial transformation that should be applied to feature map--- gives transformation conditional on the input\n- **grid generator**: parameters form part 1 are used to create a sampling grid, wihch is a set of points where the input map should be sampled to produce the transformed output\n- **sampler**: feature map and sampling grid as input to sampler, producing output map sampled from the input at the grid points\n\n## Localisation Network\n\n\n$$\n\\theta=f_{loc}(U)\n$$\n\n$\\theta$ is output, $U$ is input, $f_{loc}$ is network function, it can take any form including FC network, CNN, but should include a final regression layer to produce transformation parameters $\\theta$\n\n## Parameterised Sampling Grid\nTo perform warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at particular location in the input feature map.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/sampling_grid.png)\nAssume $T_{\\theta}$ is a 2D affine transformation $A_{\\theta}$.\n\n$$\n\\left(\\begin{array}{cc}x_{i}^s \\\\ y_{i}^s\\end{array}\\right)=A_{\\theta}\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)=\\left[\\begin{array}{cc}\\theta_{11} &\\theta_{12} &\\theta_{13} \\\\ \\theta_{21} &\\theta_{22} &\\theta_{23}\\end{array}\\right]\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)\n$$\n\n$(x_{i}^t,y_{i}^t)$ are the target coordinates of the regular grid in the output feature map.\n\n$(x_{i}^s,y_{i}^s)$ are the source coordinates in the input feature map that define the sample points\n\n$A_{\\theta}$ is the affine transformation matrix\n\nOf course, $ T_{\\theta}$ or $A_{\\theta}$ can have any parameterised form, provided that it is deffirentiable with respect to the parameters---for BP\n\n**Thin plate spline transformation(TPS) is the most powerful transformation in experiments.**\n\n## Differentiable Image Sampling\n\nTo perform a spatial transformation of the input feature map, a sampler must take the set of sampling points $T_{\\theta}(G)$, along the the  input feature map $U$ and produce the smapled output feature map $V$\n\nEach $(x_{i}^s,y_{i}^d)$ coordinate in $T_{\\theta}(G)$ defines the spatial location in the input where a sampling kernel(such as bilinear) is applied to get the value at the particular pixel in the output $V$\n\n# Experiment\nExperiments are conducted on Distorted MNIST, Street View House Number for number recognition and CUB-200-2011 birds dataset by automatically discovering object parts and learning to attend to them.\n\n## Distorted MNIST\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/MNIST.png)\n\nSpatial transformer network acts on the input before the classification network, which are FCN and CNN, all STN use bilinear sampling, but use different transformation functions: affine, projective, 16 point thin plate spline (TPS)\n\n# Street View House Number\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228784/blog/1.%20Spatial%20Transformer%20Network/street_view_number.png)\nSpatial transformer network acts on the input before baseline CNN, and define another extension where before each of the first 4 convolutional layer of the baseline CNN. Regression layers of spatial transformer are initialised to predict the identity transformation. Affine transformation and bilinear sampling kernel are used\n\n# Fine-Grained Classification\n\nST-CNN is able to discover and learn part detectores in a data-driven manner without any additional supervision\n\n# Conclusion\n\nAccuracy improved in many tasks using spatial transformer network, it would be useful for tasks requiring the disentangling of object reference frames.\n\n\n\n","tags":["Transformation"]},{"title":"Hello World","url":"/2018/09/06/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]