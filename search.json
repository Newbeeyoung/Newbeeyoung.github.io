[{"title":"激荡三十年笔记","url":"/2018/12/24/激荡三十年笔记-1/","content":"# 激荡三十年（上）\n## 时间线\n**第一部分：开放与激活（1978~1983）**\n1978年，全国科学大会，邓小平提出“科学技术是生产力”\n高考恢复\n国内掀起“真理标准大讨论”\n“家庭联产承包责任制”，农村改革\n中共十一届三中全会，停止阶级斗争\n之后很长一段时间，招商引资成政府首要任务\n1979年，邓小平把重点放在数十万家国有企业的改造上，南方试验，窗口效应吸引技术、资本\n深圳等特区诞生，土地出租协议\n“倒爷”横行\n大型国企企业自主权试验，“首钢风波”\n“利改税”，部分解决企业与主管部门抢饭吃，但一直不敢面对敏感的产权变革（直至1998年）\n1982年，邓小平决定对私营企业采取“看一看”的方针\n民间力量泉涌，邓小平：“让一部分人先富起来”\n\n**第二部分：骚动与喧哗（1984~1992）**\n1984年，第一代企业家上路，“笼子”里的国企是“老大难”\n中央宣布“向外国投资者开放14个沿海城市和海南岛”\n批条泛滥，官倒盛行，海南烂尾楼\n80年代中期中国经济“发烧”\n“价格双轨制”出台，倒爷仍旧手眼通天\n1988年，“物价闯关”，很快失控\n通货膨胀，公众恐慌，抢购成风\n“宏观调控，治理整顿”，私营企业遭政府整顿\n整顿第二步：清理整顿国营体系外的新兴企业\n1989年，邓小平提“中国的问题，压倒一切的是需要稳定”\n1990年，亚运开始经济从过热转入平稳\n邓小平提出：“开发浦东”；建立证券交易所\n关于意识形态的争论愈演愈烈\n1991年，朱镕基赴京任主管经济副总理\n1992年，加快改革和开放令中外的不信任密雾散去\n\n**第三部分：激进与梦想（1993~1997）**\n1993年，粮票取消，计划经济时代划句号\n朱镕基1）清理国企“三角债”2）民间失控的非法集资，分税制改革，梳理中央与地方财政关系；汇率改革，人民币大幅贬值3）治理国有企业，“重点扶持，其余放活”\n1994年，跨国公司铁甲轰隆而至\n市场经济崛起，老牌国企没落，90年代上海“退二进三”（进入第三产业）\n“上市指标”成为政府救活国企“最后一把米”，“意外”解决居民储蓄增长过快的“笼中虎难题”\n1996年，国家队仿效“大宇模式”，欲进军500强\n1997年，亚洲金融风暴\n市场环境骤变，民企“雪崩之年”，苏南模式遇挑战\n中共十五次全国代表大会，江泽民提出“混合所有制”概念\n“国退民进”新战略出现，国企退出164个竞争性行业，垄断上游能源行业\n\n**第四部分：震荡与博弈（1998~2002）**\n1998年，港府托市成功，股指企稳，索罗斯铩羽而归\n捍卫人民币不贬值，出口增长率下滑\n全国居民储蓄高达5万亿元，取消福利分房，拉动房产\n长江大洪水，损失2551亿元，死亡4150人\n柯达收购案通过，国企改革从经营机制转变向产权重组与清晰化迈进\n“国退民进”泛运动化和法制监管空缺，巨额财富重组成谜\n1999年，32天阳线，央行降息，《证券法》实施，“庄家”出笼，大作非流通股\n媒体爆《基金黑幕》，吴敬琏比喻股市为赌场，称必须否定“股市为国企融资服务”，“政府托市、企业圈钱”的做法\n美国导弹轰炸中国驻南联盟大使馆，《新日美防卫合作指针》\n中国加入WTO，谈判协议戏剧性达成\n2001年，中国男足进世界杯决赛圈\n国有大营企业“换血重生”1）大规模整体海外上市2）打破垄断，增强竞争的大跨度拆封重组3）企业家群体显本色\n跨国企业掀起独资化浪潮，有些转入资源性行业\n“第三力量”民营资本奋力争取生存空间\n日本通电省白皮书称中国成为“世界工厂”，廉价劳动力亦喜亦忧\n\n**第五部分：责任与理智（2003~2008）**\n2003年，温家宝上任第一天，北京接到第一例SARS病例\n中国制造与地产内外俱旺，非理性繁荣周期又至\n原材料和能源因紧缺而一路飞涨\n2004年，国务院针对地产推出一系列严厉的调控措施\n2005年，神州六号飞上太空\n全球每七件反倾销和贸易救济案就有一件针对中国\n上证指数失守千点\n股权分置改革低潮启动，异常顺利\n2006年，国有银行大规模上市，企业并购加快\n历时12年的三峡大坝完工，投资240亿美元，年均发电847亿千瓦时\n青藏铁路贯通，全长1956公里，960公里在海拔4000米以上\n天津滨海新区2270平方公里，北方的金融开放中心\n上证指数重返2000点，楼市回升明显\n2007年，阿里巴巴在香港联交所火爆上市\n中国的环境问题引起世界关注，全球20个污染最严重城市，中国占16个\n发改委安排5.4亿元国债资金支持98个重点节能项目\n国内通胀压力加大\n## 1978 中国，回来了\n1.\t在日后的几年里，它（实践是检验真理的唯一标准）与“以经济建设为中心”以及“稳定压倒一切”等政治格言一脉相通，分别从方法论/战略目标和成长边界三方面进行了清晰的表述，从而构成了中国改革文化和三大思想基石。可以说日后中国企业及其他事业的发展，折冲百回，曲线前行，都以此为最根本的起点和边界。\n2.\t法国思想家、1927年诺贝尔文学奖获得者柏格森曾说：“说社会的进步是由于历史某个时期的社会思想条件自然而然发生的，这简直是无稽之谈。它实际只是在这个社会已经下定决心进行实验之后才一蹴而就的。这就是说，这个社会必须要自信，或无论怎样要允许自己受到震撼，而这种震撼始终是由某个人来赋予的。”邓小平无疑就是柏格森说到所谓的“某个人”。\n3.\t在改革开放初年，邓小平曾经尝试用“巨额资本密集投入”的方式来迅速地拯救中国经济，这仍然是一种强有力的国家经济运动，与毛泽东发动群众搞经济的方式不同，邓小平这一次是试图接用资本主义的钱来造“中国大厦”。不过，他的这个浪漫蓝图很快就被证明是行不通的。在发现这一计划无法实现的第一时间，邓小平就迅速改变了战略，他开始把重点放在上万家国有企业的改造上，期望通过对它们的放权改造激发出生产的积极性，与此同时，他还在地理位置比较偏远、国有经济实力不强的南方进行特区试验，用窗口效应来吸纳国外资本和技术。\n## 1979 新的转机和闪闪的星斗\n4.\t国有企业改革的核心命题是什么？30年后，几乎所有学过经济学的人听到这个问题，都会很顺口地背诵出诺贝尔经济学奖获得者科斯在1959年写过的那句斩钉截铁的话：“清楚界定的产权是市场交易的前提。”因而，国有企业改革的核心是产权制度改革。\n5.\t事实上，每一个强大的个人，当他面对顽固的制度性障碍的时候，依然会表现得那么软弱无力。当袁庚（蛇口工业区管理委员会主任，负责蛇口开发）被派遣到蛇口的时候，他的领导者是希望靠他这个“强壮而精明”的武士“杀出一条血路来”。他确实完成了这个任务，而麻烦的是，他居然还想顺便完成另一项更重大的任务，在这个新开拓的土地上构筑原来全然不同的制度，这显然已经超出他的“使命”。于是，最后的落寞便已经命中注定。\n## 1981 笼子与鸟\n6.\t从总体上来看，《关于发展社队企业若干问题的规定（试行草案）》是鼓励社队企业的创办和发展的，规定还提出了很具体的指导方向。然而在章程的细节上，却可以清晰看出计划经济的痕迹，从发展的战略思想上更可以看出，中央发展社队企业主要还是为了解决农村问题。在这个规定的第二章“发展方针”中，便明确规定：社队企业必须坚持社会主义方向，积极生产社会所需要的产品，主要为农业生产服务，为人民生活服务，也要伟大工业、为出口服务（以农业养工业）。\n## 1982 春天并不浪漫\n7.\t长春君子兰事件在当年并非孤例。1982年前后，江浙一带也曾爆发过五针松（一种观赏型松树盆景）的炒卖事件，其疯魔状况也毫不逊色。这些现象颇似17世纪荷兰发生的郁金香事件。它可以被看做贫穷日久的底层民众对财富渴求的一次妖魔式释放。“潘多拉的盒子”真的被打开了。（比特币？）\n8.\t客观的说，1982年宏观经济的紧缩，并没有造成意识形态上的全面回流，它所表现出的种种粗暴是一个习惯于用行政手段和思路解决市场经济波动问题的政府，在面对新环境时缺乏市场经济管理能力的体现。在中央决策层，改革依然是主流的力量。\n## 1983 步鑫生年\n9.\t（1981-1983，年广九-傻子瓜子和陈志雄们雇佣八人以上帮工算不算违法的争论一直没停，马克思的经典论述谁也不敢违背。邓小平的策略是“放两年看”，“再看看”-实践标准。1987年对雇工数量的限制才被去掉。）\n从这个细节就可以看出邓小平领导这场艰巨变革的战略思路：摸着石头往前走，不争论，也不做政策上的明确界定，让最终发生的事实来定义前行的方向。这种改革思维使中国变成了一个巨大的经济试验场和冒险乐园，所有的激情和野心都被无限地激发出来，从社会底层喷发出来的火浆终于让大地熊熊燃烧，只要有利于经济的发展和财富的积累，一切都似乎百无禁忌，中国社会的道德底线和法制底线一次次地受到挑战和冲击，公共价值观念变得越来越世俗化和物质化。\n10.\t倒爷（因为国家统购统销，所以在物流需要指标的流通体制外建立物流网络，从中谋取差价。牟其中为首）如蚂蚁啃堤，最终把僵硬的计划流通体系摧毁得遍体鳞伤，从而以一种十分灰色而非法的方式协助重建了中国的市场流通和资源配置。他们是经济转轨期里必然出现的经济寄生物。 \n11.\t检讨整个80年代的国企改革，我们发现，在推动企业改革的重点上出现了战略性偏差：国有资本的拥有者把重点放在了内部管理制度的变革上，而事实上，当时最大的变化则发生在商品流通的环节。\n12.\t小商品的价格的放开与松动，为货物的流通和民营企业的崛起带来了可能性。中国第一代民营企业的出现与小商品的流通与制造密切相关，正是在这些不起眼的大中型国营企业不屑一顾的领域里，精灵般活跃的民间资本完成了自己的原始积累。\n13.\t义乌的发展模式，几乎是80年代中国民营经济成长的标本：一个专业市场的出现，构筑出一张辐射农村及中小城镇的商品网络，在物流需求的诱发下，周边冒出数以千计的家庭工厂，最终形成“前店后厂”、“双轮驱动“的初级产业格局。在中国改革的前十多年，任何产业基础、政策扶持、人文素养乃至地理区位等方面的客观条件，都无法与当地的改革创新意识相匹敌，往往，一地观念的解放与否是它有没有可能发展起来的唯一条件。 那些工业基础雄厚、地理位置优越的城市地区，如东北（东三省的衰落就来自对体制的高度依赖）、华北及上海等地，由于计划经济色彩浓厚，政府管治能力健全，民众对体制的依赖度较高，民营资本难有萌芽的机会，倒是天偏低远、国有经济薄弱的变穷地区，如珠江三角洲、闽南、浙江中南部一带，却意外具备了自谋生路的勇气和可能。\n14.\t一位叫张仁寿的温州研究者曾用“边区效应”解读温州地理极不便捷和发达的商品经济的矛盾：“温州十大市场大多坐落在水路交通都不是很便利的地方，唯一合理的解释只能是，在那些地方，左的思潮相对薄弱，计划经济的束缚相对较小，否则，这些市场可能在兴旺之前就遭取缔。中国改革的经验证明，对旧体制的最初突破，往往发生在旧体制最疏于防范的地方。”由此可佐证当时温州商人的处境之凶险，改革先行者的狡黠与酸楚大抵都在这里了（温州一家人）。\n15.\t执行多年的利润上缴方式，改成有比例的纳税制。“利改税”是国营企业向现代公司治理制度改革的第一个重要举措。它把企业从”父爱式“的大包大揽中解放了出来，尽管这个”解放“才刚刚开始，而且”父亲“的所得仍然是”大头“。\n## 1985 无度的狂欢\n16.\t在中国改革史和企业史上，“海南汽车倒卖事件“（全国除海南汽车进口有指标，引发汽车走私倒卖）带有很强的”寓言性“，一个地区为了发展经济，令制度的许多欠缺渐渐跟不上经济发展的需求，中国改革的渐进特征日益明显，开始进入漫长的灰色地带。\n17.\t1985年的引进热（民企大规模引进欧美日完整生产线）在日后遭到了批评。大规模的引进热浪，使得外汇消耗巨大，到年底，全国的贸易逆差达到创纪录的137.8亿美元，相当于出口总额的52%。不过客观地说，这次时空的引进热对中国轻工产业的更新换代及消费市场的启动，产生了巨大的效应。这个时候的中国公司，就好像一个青春期的少年冲进一片正在疯长中的草地，你听得到他的骨骼与青草一起向上生长的声音，过度的精力和热情挥霍似乎是无可避免的。（野蛮生长-冯仑）\n18.\t从晋江假药案开始，在广袤的中国农村，有意识、有组织、大规模地制造假劣产品，在今后20年的时间里仍将持续地蔓延，从来没有根绝。它成为很多地方摆脱贫困的捷径，成为地方政府振兴当地经济的绝招，在晋江假药工厂中，相当一部分的创办人和经营者是当地的乡镇干部，这个特征在今后也将一再呈现。中国基层社会那种流传千年的淳朴的商业道德，从这时开始正在可怕地逐步沦失。\n19.\t加快投资、加快发展再度成为国家的主题，宏观经济在沉寂数年后再度出现趋热的迹象。另一方面，全民性的物质欲望被猛然的激发出来，摆脱贫困成为至高无上的公共理想，在这个目标之下，对制度和道德的漠视受到默许，这同时也给那些保守的势力提供了攻击的机会。 \n20.\t1985年初，国家宣告生产资料的“价格双轨制“正式形成。双轨制的意思是，一种生产资料存在两种价格，一种是国家掌控的”计划内价格“，一个是市场化的”计划外价格“，后者的成本要远远大于前者。这种扭曲的价格体制，其目的是为了保护国营企业在原材料采购商的优势。(改革开放初期为短缺型经济，政府既是监管者又是竞争者。双轨制让官员及亲属利用权力获得低价的物资，倒卖到市场上获得暴利。但一步放开也不可能，毕竟计划经济还存在。在89改为单轨)\n21.\t倒爷使得国家统配物资以各种形式通过各种渠道流向市场，国家指令性计划彻底失去了严肃性。得益于这种畸形价格制度的倒爷阶层是压垮计划经济的最后一根稻草。\n22.\t社会舆论对雷雨的评价，最生动的体现出中国改革过程中的观念紊乱和制度悖论，在相当长的一段时间里，对官员和企业家们的经济行为的法律判断一直非常迷乱，甚至带有很大的随意性和阶段性。（法制建设跟不上经济发展，如果以当今的法律去评判八九十年代的企业家，估计富豪榜里面百分之九十都得进去了。不像经济发展那么迅猛，法制要一步步试错才能完善）\n## 1986 一无所有的力量\n23.\t在某种意义上，政府甚至天真地认为，当年他们交付给国营企业的那些社会责任这次可以由乡镇企业来承担了，乡镇企业兴起之后，他们理应承担起相关乡村的所有社会功能，包括就业、社会设施配套】社会公共服务等。这种想法是当时政府和社会的一个主流思想。（企业的目的就是最大化利润，承担社会责任也是为实现这一目的）\n24.\t那些善于利用和占有各种政府资源的乡镇企业迅速壮大，并以各种千奇百怪的方式完成了产权清晰化，十多年后，鲁冠球和他的万向集团变成了中国最大的私人公司之一。这是那些靠创办乡镇企业暴富起来的企业家们的“致富潜规则”——他们充分利用了各级政府的急切和天真心理，以创造公共财富和承担社会职能为理由和承诺，获得了低成本的政策扶持，与成立的国营企业相比，他们有着体制上的宽松性，同时土地成本和劳动力成本的低廉让他们具备更强的竞争力\n25.\t费孝通发表了《乡镇工业看苏南，家庭工业看浙南，温州33万人从事家庭工业》的报道，附发的评论首次提出了一个新的名词：“温州模式”。从此开始，集体经济的苏南模式与私人经济的温州模式，成为中国民营企业的两大成长模型。\n26.\t（“经济联合体”，国企把自己一些业务以承包或者联营的方式转包给乡镇企业。国企降低了成本，乡镇企业绕开管制，获得市场准入，人才、品牌、技术支持。苏泊尔是典型）事后表明，这种支援和联营，是沿海一带乡镇企业获得迅猛发展的重要原因之一，国营企业在联营中所获得的效应最终被证明只是暂时的，而灵活的民间企业则从这个体量庞大而体制僵硬的“大笨象”身上汲取了无尽的“血液”。数年之后，联营的乡镇企业迅速壮大，而被掏的资源一空的国营企业则更加羸弱不堪，于是，将很快出现“儿子吃掉老子”的现象。\n27.\t民间力量的崛起，使得原有的体制越来越无法适应，然而政策上的改变却迟迟不至，于是，对现行法律的违背与穿越变成改革者不得不为之的冒险行为。（如温州地下钱庄得到地方政府默认，却得不到上级银行的认可，处于非法状态。）\n## 1987 企业家年代\n28.\t马胜利（承包国营纸厂）的承包旋风和集团化梦想，便是在这样的宏观背景下，这是那个时期国营企业经营者最富想象力的实验，它试图通过承包，也就是市场化的手段，以一人之力拯救百家亏损企业。马胜利的失败，对那种“一改就灵”、“一包就活”的改革理念是一次意外而重大的打击。（一改就放，一放就乱，一乱就收，一收就死，一死又放，哈哈哈）\n29.\t很多时候，“准确的预见”对于企业家来说是一种莫名的天赋。就好像王石在土地拍卖中窥见了房地产业的曙光，在北京柳传志在喧嚣中看到了个人计算机的方向。\n30.\t在这一年的企业故事里我们日渐感受到越来越多的商业气质，企业家如马胜利、李经纬、柳传志开始真正运用商业的手段和规律来经营一家企业。经过近十年的曲折发展，中国的消费市场逐渐放大，从民间崛起的力量开始展现他们的能力，这也为现代企业的出现创造了最好的土壤。（虽然现在联想饱受争议，但说柳传志是中国“企业家教父“也没问题）\n## 1988 资本是苏醒\n31.\t在后来的几十年里，这一直是很多国有企业在悄悄尝试的办法：通过引进私人投资者的方式，组建一个产权清晰的子公司，以此形成一个新的资本操作平台，来推动乃至完成母公司的资本改造。\n32.\t（中央放开管制，取消物价双轨制，导致物价大涨。民众看涨物资进行抢购，诱发进一步通货膨胀）“物价闯关“被认为是1978年改革以来最大的一次经济失控，它在10月份就宣告失利，中央开始调整政策，再次提出”宏观调控，治理整顿“的方针。此次失利，表现为商品抢购和物价飞涨，它对于宏观经济所产生的影响虽然是负面的，但并没有招致毁灭性的生产崩溃，然而它对全国民众的改革热情则是一次重大的挫败，在通货膨胀中受到损害的民众对”价格双轨制“下大发横财的”官倒“更为痛恨，并由此产生了”改革造成社会不公“的印象。\n## 1989 “倒春寒“\n33.\t88年的通货膨胀导致89年进行宏观调控，政策紧缩是企业间正常的货物和资金来往瞬间紊乱，资金的循环拖欠构成了怪圈，即“三角债“。”三角债“的解除要等到93年经济再次复苏才逐渐完成，在客观上，他是经济结构严重失调和银行紧缩银根所造成的，而在深层次上，则直接映射出全社会信用体系的重大危机。从几年前的晋江假药案以来，受到商业利益的驱使，以及没有相应的法律及时予以约束，公众的价值判断开始发生紊乱，全社会的商业道德终于露出来塌陷的裂缝。\n## 1990 乍热骤冷\n34.\t早期温州商人在商业上表现出来的智慧和狡黠，与硬骗强取不同，却将原始积累过程中的那种没有道德底线的狡黠与血腥，展现得淋漓尽致。伴随着市场繁荣和民众富裕的是，原有商业秩序的破坏、淳朴的公共观念（文革后还有吗？）的淡漠和生态环境的破坏。我们只能说，那是一个缺乏善恶感的时代，在“摆脱贫困“这个时代主题面前，一切的道德价值评判都显得苍白无力。\n## 1991 沧海一声笑\n35.\t1988年的物价闯关失利以及发生在1989年的社会动荡和经济低迷，让中国的决策者对未来的改革模式有了新的思考。激进的思路渐渐退潮，一种渐变式的改革理念成为新的主流。1989年，邓小平曾提出，中国的问题，压倒一切的是需要稳定。《人民日报》在新年元旦社论中写道，“只要保持稳定，即使是平平稳稳地发展几十年，中国也会发生根本性的变化。“（自由和稳定能够共存吗）\n36.\t中国改革史的一个特点便是，人民的实践有时候会走在中央政策的前面，一切的改革和突破尽管会阶段性地受到意识形态争论的影响和干扰，但是并不能真正地阻挡它前行的脚步。\n37.\t对陷入困境或处于市场边缘的国营资产的分割、重组与出售，将成为财富积累和改革成果分配的重要方式。如果说，在1988年，“资本“这个曾经被视为洪水猛兽的万恶名词已经重新归来，那么在今年，他已经散发出越来越迷人的金色光彩，那些最早认识到它的人将一一成为新的财富宠儿。而让人感到意外的是，这些人中的相当一部分正式从庞大无比又貌似僵化的国营资本这座”大金山“上，挖取到了各自的“第一桶金”。（牟其中牵线前苏联和四川航空，用500车国营企业罐头皮衣等积压商品换取4架苏制飞机，从中赚了8000万到一个亿；仰融以1200万美元控股资本质量良好却暂时陷入困境的沈阳金杯，在百慕大设立华晨控股，带上国营资本的“红帽子”，成为中国第一家海外上市企业；宗庆后以区级校办工厂兼并濒临停产的全国第四大罐头厂-杭州罐头厂，一百天扭亏为盈。）\n38.\t莎士比亚：世事的起伏本来就是波浪式的，人们要是能够趁着高潮以往直前，一定可以功成名就，要是不能把握时机，就要终身蹭蹬，一事无成。\n39.\t在企业史上，陈光被称为国企产权制度改革“第一官”。自80年代初开始的国企改革，仍是以改善政府部门和企业间的关系为主，从放权改革到承包制，各地政府和经营者尝试了无数种改革的模式和方法，但却始终没有初级最致命也是最敏感的产权制度改革，企业的产权关系一久不明晰。用陈光的话说，“还是工人当家不做主，厂长有权不落实，企业盈亏不负责”。\n## 1992 春天的故事\n40.\t邓小平在视察期间的讲话后来都被整理成文——“基本路线要管一百年，动摇不得。”“判断各方面工作的是非标准，应该是主要看是否有利于发展社会主义社会的生产力，是否有利于提高人民的生活水平。”“社会主义的本质，是解放生产力，发展生产力，消灭剥削，消除两极分化，最终达到共同富裕。”“计划多一点还是市场多一点，不是社会主义与资本主义的本质区别。”“改革开放胆子要大一些，抓住时机，发展自己，关键是发展经济。发展才是硬道理。”“中国要警惕右，但主要是防止‘左’。“”要坚持两手抓，两手都要硬。两个文明建设都搞上去，这才是有中国特色的社会主义。“这些讲话的核心其实便是，对无所不在的意识形态争论给予了断然的”终结“，他似乎已经没有耐心继续在”理论“的层面对那些纠缠不清的问题进行讨论了。（“南巡”直接促进了十四大社会主义市场经济的确立，掀起经济发展的第二波高潮）\n41.\t在中国的改革史上，“邓小平南巡“是一个重大事件。在有些时候，它甚至被认为是一个历史性的转折点。从1978年改革开放以来，中国的发展主轴已经向经济成长转移，然而围绕经济领域中出现的种种新现象，仍然有不少人以意识形态的标尺去丈量和批评。每当宏观经济出现波动的时候，便立刻会有批评和指责的思潮出现。这已经成为阻碍中国经济持续增长的最大的思想屏障。（保守派）\n42.\t92派是中国现代企业制度的试水者，和之前的中国企业家相比，他们应该是中国最早具有清晰、明确的股东意识的企业家的代表，这些人往往在政府部门待过，有深厚的政府关系，同时又有一定的知识基础，具有前瞻性的预测能力，创立一个行业并成为行业的领头羊。（92派指大批在政府机构、科研院所的知识分子受南巡讲话的影响纷纷主动下海创业。代表有陈东升（泰康人寿）、田源、冯仑、王功权（鼎晖创投）、潘石屹、易小迪（阳光一百）。\n43.\t“破三铁”，（铁饭碗、铁交椅、铁工资）是企业改革15年来，第一次把改革的矛头对准了企业中的一般职工，在此前，所有的改革理念和措施都是针对经营层与国有资产管理层的。”破三铁“，其实也就是解除了企业与工人的”终身劳动契约“。在某种意义上，大张旗鼓的”破三铁“是一次无可奈何的观念运动，它让人们意识到，他们一直以此为家的国营企业不再是永远的保姆和不沉的大船。三铁既破，社会保障体制却没有健全，成千上万的工人下岗，引发社会危机。到年中，运动就戛然而止。\n44.\t“大中华区“既不是一个政治实体，也不是一个组织有序的贸易区，但它却在同一种文化和共同对发展渴望的驱动下，练成一体。它整合了中国台湾的技术和财力、中国香港的国际市场经验和中国大陆巨大的土地、劳动力资源还有野心。\n45.\t1978年以后，国家的成长主题从主义之争重新回到经济发展的轨道上。在十多年之后，当互联网这个幽灵从美国东海岸的实验室里窜将出来，搅乱整个商业世界的游戏规则时，已经初步完成了体制和观念转轨，特别是形成了一定的民间资本力量的中国正好踩在了这个转型点上。很难想象，如果中国的经济改革再迟十年，或互联网的浪潮提前十年到来，中国的今天和未来将会是一番怎样的格局。（2017年中国IT及电子产业的研发投入国内第一，达2000亿元）。\n46.\t1992年是一个新阶段的起点。当市场经济的概念终于得到确立之后，面目不清的当代中国改革运动终于确立了未来前行的航标，改革的动力将从观念的突破转向制度的创新。在之前，人们认为，中国之落后主要在于科技，只要大量地引进生产线和新技术，就能够很快地迎头赶上。而现在，很多人已经意识到，观念突破和技术引进所释放出来的生产力并不能够让中国变成一个成熟的现代国家，经济学家吴敬琏因此提出“制度大于技术“。\n在此之后，我们即将看到，中国开始从观念驱动向利益驱动的时代转型，政府将表现出热烈的参与欲望和强悍的行政调控力，国营、民间和国际三大商业资本将展开更为壮观和激烈的竞争、博弈与交融。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 激荡三十年（下）\n## 1993 扭转战局\n1.\t朱镕基在经济治理上的强势风格，在对金融秩序的维护以及对禹作敏和沈太福俩案中得到了充分的展现。朱镕基开始用一种更专业、更具体细微的方式来管理中国经济。朱镕基自1991年从上海市委书记调入北京担任主管经济副总理（现在是刘鹤），并在此后10年内主导中国的经济，并成为继邓小平之后，对中国经济改革影响最大的政治家之一。（之一？）\n朱镕基到京后的“第一战“是清理三角债。26天清理拖欠款125亿，东北问题基本解决。到1992年五月中央和各地政府、企业数年的“三角债”被解开。清理“三角债”首战告捷之后，朱镕基在金融领域开打“第二战”，他在“金融政策“的调控上再显铁腕手段。首先，他亲自兼任央行行长，下定决心清理金融领域的体制外活动（沈太福集资10亿元，并因为人民银行关于其非法集资的通告向人民银行行长索赔1亿元），对于任何有可能扰乱现有金融秩序的行为都严惩不贷。其次，他提出分税制（1992分税制实施前，中央财政赤字1000亿元，分税制指中央与每一个省份磋商分税种类和比例，中央财政收入站全国财政收入比重从1993到2002上升33个百分点），重新梳理中央与地方政府的财政关系。朱镕基另一个具有深远意义的金融决策是，力排众议实行汇率改革，让人民币大幅贬值，使得中国商品便宜了，中国更加吸引外部投资商，促使中国经济走上外贸拉动型的道路。朱镕基主政的”第三战“，也是贯穿十年的治理主题，是搞活国营企业。\n2.\t华裔经济学家黄亚生在《改革时期的外国直接投资》一书中断定，“对私人企业的歧视增加了中国对外资的需求“。他认为，在整个20世纪90年代，国有部门是在破产的边缘上运营的，而与此同时，私人企业尽管得到了迅猛的发展，但是仍然受到严重的信用约束，无法获得金融上的扶持，并被限制进入很多行业。没有效率的国有资本需要寻找有效的合作资本，它面对的是两大资本集团，一是跨国资本集团，一是民间资本集团，正式在这种抉择中，外来的跨国资本成为被选中的对象。因此，1992以来，外资的大量涌入，与这种改革思路的确立是分不开的。\n## 1994 青春期的躁动\n3.\t那些在改革开放初期创业的企业已经由草创期迈入了成长的青春期，尤其是那批在1984年（1981-1984经济降温）创办的公司，已经到了具有标志意义的第十个年份。青春期内的企业家意识到，自己必须向职业化转型。在此之前，他们往往是那些天生敢冒风险同时有注重实际结果的人，富有创造力而无所顾忌，而此时，他们面临改变。随着企业规模的壮大，制度和管理成为新的瓶颈。\n4.\t在相当长的时间里，中国电脑以及家电产业一直在“贸易“、”制造“与”技术“的发展有限次序上摇摆徘徊。联想跟中关村的所有公司一样，都是靠贸易起家，逐渐形成工业制造能力，进而在技术上寻求进步，而家电业的海尔、长虹及科龙等公司都是从引进生产线起家，然后在市场营销上获得了成功。因此，”贸工技“和”工贸技“是两大成长模式，其中无一例外的是，技术都是核心能力中最薄弱和滞后的一环。而当这些企业逐渐壮大，与跨国公司在中国市场上展开正面竞争的时候，技术落后的现状便非常清晰地显现出来。在此刻，何去何从，敏感而无比关键。在中国企业史上，柳倪之争带有很强的寓意。它展现了中国企业家在面临国际化竞争和技术发展瓶颈的时候，做出了怎样的思考和选择。一个无法回避的事实是，在20世纪90年代中后期，几乎所有知名的企业家都选择了柳式道路。（后遗症就是现在产业结构升级的艰难，典型的例子就是中兴）\n（贸工技，就是贸易（流通）-工厂（生产）-技术(科研)，简言之就是市场需要什么就生产什么，技术研发方向以市场需求决定，市场第一。工贸技相似，先引进生产线，再销售。而“技工贸“指先开发新技术、新产品，加工生产后再销售，使科技成果市场化，科技第一。）\n## 1996 500强梦想\n5.\t1996年是30年企业史上最激情四射的年份之一。每一个行业都充满了无数的商机，所有的人都变得迫不及待，扩张、再扩张，企业家们还远远没有学会控制自己的欲望。日后的事实将证明，在多元化的浪潮中看看，那些失去理智和控制力的企业家都将自食其果。\n## 1997 世界不再令人着迷\n6.\t中共十五次全国代表大会中江泽民在报告中对传统的公有制理论做出重大修正，提出了“混合所有制”概念，认为非公有制经济已经不仅仅是“补充”，而且是“重要组成部分”，国有经济的比重减少一些，不会影响社会主义性质。此次姓“公“姓”私“的争论是1978年（关于真理标准的讨论）、1992年（关于姓”社“姓”资”的讨论）之后第三次重大的思想交锋，而十五大的召开则表明中国开始了第三次“思想解放”。在中央高层和社会改革力量的推动下，一场大规模的产权清晰化运动如期而至。\n7.\t苏南模式的终结（苏南地区90%以上的乡镇集体企业通过各种方式“改制”成为私营企业），意味着集体经济的一次脱胎换骨，在更广泛的时空跨度内，它标志这自20世纪50年代开始试验的合作社制度及后来的人民公社制度在基层经济组织中的彻底淡出。一种更为市场化的、以产权人格化为特征的企业制度终于成为主流的企业成长模式。\n8.\t亚洲金融风暴击碎了人们对日韩财团企业模式的膜拜（大宇集团陷入债务危机）。一种新的“国退民进”的新战略出现了，它的基本思路是，国有资本从完全竞争领域中大面积退出。在一份报告中，专家们建议国营企业应该从164个竞争性行业中“坚决撤出”，同时在上游能源型行业中强势地形成垄断格局，这些行业包括钢铁、能源、汽车、航空、电信、电力、银行、保险、媒体、大型机械、军工等。在这些领域，政府将竭力排斥民间及国际资本的竞争，通过强化垄断来保证国营企业的既得利益，作为国营资本的所有者，国营企业的角色不是被削弱而是更为增强了。在随后的10多年里他一直被坚定地执行着，并最终把中国带进了一个国家商业主义的时代。（这种垄断并非一家独大，而是多家国企垄断整个行业，内部进行竞争，保持了一定的活力。如今在投资银行，铁路，电力等领域实施更加开放的外资政策，希望把外资的活力注入上游行业）\n9.\t史玉柱是这部企业史上最具争议性的人物。毫无疑问，他是这代企业家中市场直觉最好的人之一，他能迅速地找到行业爆发的时间点，并且以最为快捷和高效的方式获得成功，他因此被成为“史大仙”，甚至得到很多高傲的同辈企业家的崇拜。他的经历起伏和永不言败的精神则在万千大学生中产生巨大共鸣，被视为创业偶像和精神领袖。而同时，他在营销手段上的恶俗和对人类贪婪面的利诱，则受到重大的道德质疑。史玉柱自称是“中国最著名的失败者”，因此，为了向世界证明自己，他甚至把这个目标自我崇高化，不择手段，最终蔑视社会的道德底线。这种商业成功，充满了野性的血腥、冷酷和道德麻木。史玉柱的身上，投射出这个商业年代所有的矛盾。（脑白金，黄金搭档，巨人网络《征途》）\n## 1998 闯地雷阵\n10.\t1998年停止实行了40多年的实物分配福利房的做法，推行住房分配货币化。消费信贷刺激了家庭的住房需求，而大规模的基础设施建设则释放着持续的投资品需求。大量的企业也就是在这之后进入投资扩张时期的。由于投资旺盛，整个经济对于上游基础部门的能源和原材料的需求保持了持续的增长，这为大量地处上游的国有企业提供了有利的市场环境。\n11.\t中国公司的变革如果按主题来分界的话，可以从1998年一刀切为二，此前的主题是经营机制的转变，此后则是产权的重组与清晰化（https://wiki.mbalib.com/wiki/%E4%BA%A7%E6%9D%83%E6%B8%85%E6%99%B0），有区别的是，前者尽管成效甚微，不过，政策设计确实非常清晰的，而后者的改革，效果卓著但政策界定始终混沌不清。“国退民进”运动从1997年开始试验，1998年大规模推广，一直到2003年进入尾声，它意味着20年来以机制转换和放权搞活为主题的所有制格局位置一改，从而也深远的影响这日后相当长时间的中国经济。\n12.\t国有资产的大释放在某种意义上确实成了一条制造千万富翁乃至亿万富翁的生产线，后来在各种财富榜上出现的富豪们相当一部分都是这场革命的最大获利者。因而，它被人成为“最后的盛宴”。\n13.\t（褚时健作为红塔集团的董事长贪污700万元，却受到媒体同情）“褚时健”现象是一面镜子，照出了转型时期的中国商业界在法制观念和价值评判上的模糊、矛盾与迷茫。\n## 1999 庄家“恶之花”\n14.\t一个尚在空中的“西部软件园”让宋如华茅塞顿开，他突然发现，中国商业的游戏规则实在是非常神奇，有时候，你辛辛苦苦做好一个产品，不如在某个夜晚喊出一个新概念，财富的聚与散往往随着大势的摇摆而动。（这不是贾跃亭和锤子吗）\n15.\t在互联网诞生之前的所有产业中，后发的中国公司都已经没有了“创造未知”的可能性，在每一个制造业领域，我们都处在产业的末端，以能源消耗、环境破坏、劳动力价格低廉为特征的“世界工厂”的命运从一开始就冰冷地设定在中国发展的道路前方。只有在互联网的世界里，中国公司找到了打破疆域和重建游戏规则的可能性，这种可能性尽管及其微小，但是它确实存在。（中国IT电子行业确实出现了许多世界一流的公司，但中国制造需要在更多行业打破游戏规则）\n## 2000 曙光后的冬天\n16.\t1998年一直处于独家垄断的中国石油石化行业进行了一次大重组，组建了中国石油、中国石化两大集团公司。接下来它们除了把现有加油站收入囊中之外，还以国家利益的名义实施两大垄断性战略，一是获得新建站点的垄断资格，二是对民营油田进行大规模、强制性的收编和排斥。在实现对油田资源和销售渠道的双重控制之后，中国的两大石油公司则加快了海外上市的步伐以及与全球寡头石油公司的合作。经过这一系列十分强势、有计划而高效率的战略调整，它们相继成为“中国最赚钱的企业”。发生在石化领域的这场令人炫目的大变局十分生动地体现了垄断领域发生的两个变革逻辑：第一个是“以国家的名义垄断，以市场的身份盈利”，在资源性行业内形成国企之间的“内竞争格局“，民营资本被全部排斥在游戏之外；第二个是在垄断前提下加快资本化运作以及与寡头式跨国资本的结合。这样的”中国故事“发生在所有国有资本控制的垄断行业。（专制带来效率，垄断产生效益，但这种垄断的暴利是不是国家给人民上的一种隐形的税？）\n17.\t在中国企业史上，国有企业的经营者们是非常独特而值得研究的群体，他们身陷僵化的体制，肩负一个“不可能完成的任务“，却在用毕生的精力和智慧试图将自己管理的企业带入市场化的轨道。倪润峰与张瑞敏、柳传志、潘宁等人，均是靠市场开拓而崛起的企业家，他们的企业尽管属于国家或集体，实则都十分弱小或陈旧，全凭其企业家创新精神，披荆斩棘，终成一时翘楚。然而，这些新型国有企业家都面临共同的困扰，体制、产权、决策监督、企业成就与个人利益，这些话题如一个个庞大而难解的谜团让这些国字号的当家人们日日苦恼。少数先知先觉者及侥幸者逃出了藩篱，大多数成了变革的牺牲品和实验品。在中国商业界，国有企业的经营者应该是个人素质最为优异、责任心也相当强的一群，然而客观地说，在过去三十年里，除了垄断性产业之外，鲜有真正的成功者，而且也无法总结出带有普遍性的成功定律。\n18.\t2001中国年，申奥成功、国足出线、加入世贸组织。\n## 2002 中国制造\n19.\t春兰改制（把25%的集体股份出售给员工，被怀疑为国有资产流失）的叫停，在产权清晰化运动中是一个标志性的事件。此后，那些与它情况非常类似的大型企业，如海尔、长虹及海信等的改制方案都被一一搁置。在这场产权运动中，“可惜了”的显然不只李经纬、潘宁和陶建幸。由于没有制度上的保障和规范，几乎所有企业的产权变革都变成了一场巨大的冒险，是与非、合法与非法往往没有清晰的界限，企业家的命运突然变得无比的凶险和莫测起来。\n20.\t在未来相当长时期内，对于企业产权改革的讨论将成为中国公众社会及政商学界观点分歧最严重的经济话题。其中有两个重要的争议焦点，一是如何看待“国有和集体资产的严重流失”，而是如何看待国有或集体企业的经营者“赎买”。这场关于流失的争论从1998年就隐约开始了，将在2004年出现了十分火爆的激辩场面。\n## 2003 重型化运动\n21.\t自1991年临危受命赴京出任主管经济的副总理，到1998年正式接任总理，朱镕基用专业和强势的方式全面改造了中国经济，在他的任内，宏观经济一直安全地行走在“三八线”内，即通货膨找不超过3%，国内生产总值增长始终高于8%。正是这种持续的高速成长让中国在动荡的世纪交替年代保持了“风景这边独好”的繁荣景象。他通过“分灶吃饭”，彻底改变了中央与地方的财政收入格局，进一步加强了中央集权的能力。他在国有企业的改造上更是取得了出人意料的成效，在“抓大放下”和“国退民进”战略的坚决推行中，一向萎靡的国有资本集团获得了近乎脱胎换骨般的改观。1998年，当他宣布将在任内完成对大中型国有企业的改造任务时，国内外一片质疑，而最终的事实却证明他用自己的方式兑现了承诺。就在出任总理的第一次新闻发布会上，他曾在回答凤凰卫视革者吴小莉的提问时，有过“不管前面是地雷阵还是万丈深渊，我都将一往无前，义无反顾，鞠躬尽瘁死而后已”的慷慨之言，感动全国。人们均以为总理是针对改革推进之艰难而言的，如今思之，却恍然有新的感悟，其实，彼时的决策人也对改革的前途充满了巨大的莫测感。在30年的企业变革史上，朱镕基是继邓小平之后最有影响力的政治任务之一，如果说邓小平以开放的胸襟决定了中国变革的方向，那么，朱镕基则完成了路径的选择。在今后十年乃至更长的时期，中国企业一直行走在他设定好的变革逻辑中。\n22.\t1998年10月7日，出任总理不久的朱镕基去中央电视台视察，在当时国内批评之声最尖锐和最有影响力的《焦点访谈》栏目组，他题字“舆论监督、群众喉舌、政府镜鉴、改革尖兵”，并称“我也接受你们的监督”。\n23.\t中国经济不可遏制的成长态势到底是靠什么支撑的？答案来自两个方面，一是“中国制造”的外贸强劲拉动，二是以房地产为龙头的内需市场的旺盛。（经历过08次贷危机，西方国家中低端消费市场的潜力快要被挖掘殆尽，“中国制造”的红利正逐渐消失；房地产投资属性占比过重，房地产的泡沫促进贫富差距扩大，低收入的边际消费欲望大于高收入人群，贫富差距过大使整体消费欲望不强。但中国经济必须从外向型转为内需出口平衡，国内庞大的消费市场是下个阶段经济发展的主要推动力，“一带一路”出口重工业也是一种扩大外贸的战略）\n24.\t在供地方面，政府在20世纪90年代改革了供地政策，政府通过出让国有土地收取土地出让金；在征地方面，各级政府仍然沿用计划经济的办法低价甚至强行征地。左手通过权力低价征地，右手仍然是通过权力再以“市场化“的方法出让土地，其实质就是”卖你的地，挣我的钱；征得越狠，挣得越多“。粗略估计，在2002-2004年的3年中，全国土地出让金收入累计达9100多亿元，征地卖地已成为地方政府最为重要的”财政支柱“，它们也成为房价逐高的最主要的推手。被征土地的收益分配，依次是房地产开发企业、地方政府、村级组织和农民。\n25.\t2003年“中国制造“和房产热使上游能源需求激增，中国经济和产业结构发生了一个十分重大的转型—从轻型化向重型化的跃迁。\n26.\t国资委（后被并入发改委）的成立及相关政策的出台，表明在决策人士心目中已经形成了一个“理想“中的企业格局：靠”轻小集加“起家的民营企业在产业下游的完全竞争领域获得生存和发展的空间，而大型国有企业则全盘控制上游的若干垄断性行业，如此”楚河汉界，泾渭分明“。\n27.\t中国经济改革，向来有“闯关“的传统，即所谓”看见绿灯快快行，看见红灯绕开行“。很多改革便是在这种闯关中得以成功实施，在日后流传为美谈，也有不少在这个过程中黯然落马，成为违法的典型。这种改革发展与制度设计的落差，成为贯穿中国企业史的一个灰色现象。（政策、制度大于一切）\n28.\t2001年10月，李彦宏推出了全新的搜索服务“搜索引擎竞价排名“，把盈利来源直接对准了广大的中小企业，它们只要付出几百元的推广预付金，就能让自己的网页更容易被搜索到，这种介于点击广告与电子商务之间的服务让百度一下子撞开了盈利的大门。（百度的竞价排名是在骨子里的）\n## 2004 表面的胜利\n29.\t从20世纪80年代初期以来，中国历次宏观调控都有相同的“规律“，那就是：经济过热造成能源的紧缺，引发激烈的争夺，于是中央政府通过行政手段对不同所有制企业进行调控和再分配。而在这种调控和再分配中，国有企业、跨国企业以及民营企业获得的”等级“不同。这种现象几乎每隔三到五年就会出现一次，形成了30年来的经济周期。2004年的宏观调控显然也没有偏离这样的政策逻辑。中央政府进行宏观调控的依据是，宏观经济出现了令人担忧的过热景象，特别是在重化工业领域，投资增长速度到了非控制不可的地步。\n30.\t这一连串紧缩政策的组合拳出击和强大的舆论营造，不仅改变了投资者的收益预期和消费者购房的价格预期，而且改变了政府对房地产发展的支持理念和支持方式，从而直接导致了购买力的迅速下降和楼市成交量的激素萎缩。房地产的冬天突然降临。（18年何其相似，又进入到经济周期的去泡沫阶段）\n31.\t“宏观调控从来都是一种利益分配。当前宏观调控反映出来的深层矛盾是中央与地方利益之间的博弈：一方面，中央害怕地方、企业和银行联合起来骗中央，害怕物价全面上涨，害怕承担可能过热的后果；另一方面，地方和民间却希望搭上本轮经济增长的快车，特别是要赶在中央关门之前挤进门去，这大大加剧了目前的投资扩张态势，至于投资过后导致的过剩与经济下滑的威胁，并不在地方和企业考虑之内。” 显然，在这样一种博弈格局下，中央不搞宏观调控，则最终的呆坏账要由中央银行来买单，中央搞调控，所带来的损失和成本则几乎完全由地方来承担。在利益关系错综复杂的博弈过程中，中国经济发展的深层次矛盾一览无余，庞大的国有资本集团以及相关联的地方政府利益已经形成了一股惊人的“挟持力量”。（地方和企业是相对短视的，有顺周期的作用；中央要做逆周期的调控，防止经济过热或过冷）\n32.\t郎顾之争。（https://wiki.mbalib.com/wiki/%E9%83%8E%E9%A1%BE%E4%B9%8B%E4%BA%89）\n33.\t人们拿他（王石）与喜欢做秀的美国地产大亨唐纳德特朗普相提并论。（：））\n34.\t经济观察家王巍用“江湖企业家”来形容陈久霖式的国有企业经营者。这类企业家“高度迎合市场需求，积极勾兑政府资源；巧妙地利用多种身份获益，刻意的回避所有规则；既无视公司治理规则，也回避政府的组织制约；成则安生立命实现个人抱负，败则振振有词推诿于传统体制的束缚。”…中航油这一类具有垄断特权的企业应当从陈久霖的背后走上前台，接受市场的质疑。与无数在市场上艰难竞争的中小企业群体相比，正是中航油这样的垄断企业群体才可能成为威胁市场秩序，颠覆政府规则的主力，才是构成“江湖企业家”最深厚的土壤。（一方面来自国企产权不明晰，有政府背书；另一方面来自垄断）\n## 2005 深水区\n35.\t2005年4月29日，证监会发布《关于上市公司股份分置改革试点有关问题的通知》，标志着股权分置改革试点工作正是启动，它被定义为“中国股市的第二次革命”。\n2006 资本的盛宴\n36.\t在高速成长的通道里，大型国有企业的光芒最为耀眼。自2004年春夏的宏观调控之后，它们在资源型领域中的垄断地位得到了空前的巩固。如果说中国经济发展是一颗结满了苹果的大果树，那么，它们无疑是最大的、在有些繁茂枝条上是唯一的收获者。同时，随着现代企业制度的推广，这些企业的资本市场化和竞争力也得到了加强。\n37.\t2004年11月1日，一个十分戏剧性的换岗新闻轰动全球商业界。在国资委的主持下，三大电信企业的领导者换岗任职。\n38.\t9月国家统计局最新调查结果显示，国有企业集团总资产近20万亿向垄断领域集中。在娱乐业、计算机服务业和建筑装饰行业等行业中，没有国有及国有控股集团。在一些市场化程度较高、竞争激烈的行业，国有及国有控股企业集团所占比重相对较低，如木材加工、服装和建筑安装等行业低于10%，纺织业、农副产品加工业、塑料制品业、化学纤维制造业比重不到1/3.而在石油和天然气开采业、电信和其他信息传输服务业、煤炭开采和洗选业几乎全为国有及国有控股企业集团所占据。电力、热力的生产和供应业、运输业、交通运输设备制造业等关系国家经济命脉的关键行业或领域中，国有及国有控股企业集团所占比重也在90%以上。从1998年到2005年，国有企业利润节节攀高，实现利润从213.7亿元提高到9047亿元，短短7年增长了42.3倍，累计利润超过4万亿元。以此视之，国有企业效益困局不但豁然全解，而且呈现出前所未有的强盛。（垄断的力量）\n39.\t冯仑在《跨越历史的河流》中写道：民营资本从来都是国有资本的附属或补充因此，最好的自保之道是要么远离国有资本的垄断领域，偏安一隅，做点小买卖，积极行善，修路架桥；要么与国有资本合作或合资，形成混合经济的格局，以自身的专业能力与严格管理在为国有资本保值增值的同时，使民营资本获得社会主流价值观的认可，创造一个相对安全的发展环境。今后，随着和谐社会的建立和发展，民营资本将以数量最多、规模小、就业广、人数多为特征，其生存空间将被局限在与国有资本绝无冲突或者国有资本主动让出的领域。面对国有资本，民营资本只有始终坚持合作而不竞争、补充而不替代、附属而不僭越的立场，才能进退裕如，持续发展。\n## 2007 大国崛起\n40.\t在持续高速发展的同时，高污染与高能耗成为大国崛起中两个令人尴尬的伴生物。\n41.\t股市与楼市的空前繁荣，意味着在“高速公路”上持续前行了20多年的中国经济又驶入了一个加速度的周期，它带来了多重的社会景象：国家及个人财富的重新分配，中产阶层的空前扩容与活跃，全民投机心态的扭曲，中国公司的市值膨胀，宏观经济的泡沫化加剧等。在过去的30年里，2007年是社会资本最为活跃、财富分化现象最为显著的一年。\n42.\t《金融时报》记者描述了西方世界的矛盾心态：“一方面，西方消费者获得巨大好处，另一方面，西方人大声抱怨中国商品正在让当地人失去工作，让中国得到不容置疑的好处。”\n43.\t过去15年，互联网在中国从无到有，肆意成长，终于自成一体，并渗透到了经济生活的每一个细胞。尤其值得骄傲的是，在这个世界里，本土企业几乎在所有的领域都击败了各自的国际对手（Google，amazon, uber…），这在其他行业是从来没有出现过的完胜场面。另一个很奇异的事实是，凡是被国际公司收购的企业都前景堪忧(3721, 卓越…)。（至今如此）\n44.\t当这个国家开始改革开放的时候，他们还没有来到人世，现在却已经开始颠覆所有貌似强大的东西，这是天生的全球化的一代，是生来就与互联网“无缝对接”的一代。他们成长在一个开放的家庭和社会，没有经受过意识形态的煎熬，没有传统的羁绊和包袱，更没有产权制度的困扰，他们比所有的前辈商人都要幸运，他们看上去一个个前程远大。不过，他们即将面临的挑战和灾难也是前所未有的。他们的时代是最好的，也是最坏的。意大利思想家马基雅维利在五百多年钱就告诫说：“追求梦想的人们啊，已经付出就要准备付出更多。”所有的商业故事其实都符合一条规律———就如同发生在这部企业史中的每一个悲喜故事那样——伟大是熬出来的。\n可以预见到是，在未来的岁月中，如果中国要诞生世界级的伟大公司、出现取得世界性声誉的中国企业家，互联网也许是仅有的领域之一。（马云爸爸）\n45.\t股市之狂热、楼市之飙升、人民币之升值、通货膨胀之隐患、贸易摩擦之激烈以及大国情结之高昂，每每让人想起那句名言，“历史往往是重复的，只是经常以另一种方式呈现出来罢了“。人们不由自主地将今日之中国与20世纪中后期的日本相提并论。\n\n","tags":["杂书笔记"]},{"title":"Dynamic Programming Summary","url":"/2018/10/08/dynamic-programming/","content":"\n**I have spent dozens of hours in last month on Dynamic Programming, it is time to go through all the questions and make a conclusion. I believe a good summary would be as important as finishing 50 questions.**\n\n# Basic Concept\n\nIn my view, **dynamic programming** consists of two key concepts: \n1. State\n2. Recursive or iterative relation between state(Transition Function)\n\nState, some time could be regarded as storage, which leads to **Memoization** and **Tabulation** in dynamic programming. Current state could be generated by previous state using transition function.\n\nThe key of solving dynamic programming problem is always finding the **relation**. Of course, it requires a lot of practice. \n\n# Basic Method\n\nMost of the blogs and tutorials in dynamic progamming would introduce 2 major methods:\n\n1. Memoization - Top down\n2. Tabulation - Bottom up\n\nHere is a great article explaining these 2 methods:\nhttps://www.geeksforgeeks.org/tabulation-vs-memoizatation/. In simple words, both of them have tables storing state. Top down with recursive method, will use table entry directly if it is not null, otherwise it will fill the table entry, make it available in the next recursion. Bottom up with iterative loop will fill table entry from left top to right bottom. The latter states depends on previous states.\n\n# Summary of Leetcode questions\n\nHaving finished most of **EASY** and **MEDIUM** questions in dynamic programming in Leetcode, i summarized all the questions to find some hidden regularities.\n\n## Type 1 - Bottom up:Time Complexity O(N) Space Complexity O(1)\n\nThe optimal solution for ith item is only determined by most recent states, so not all previous states need to be stored. For example, in \"House Robber\" problem: \n```\ndp[i]=max(dp[i-2],dp[i-3])+house[i]\n```\nIn this case, dp[i] must contains house[i], which means robber must rob house[i],so we need to compare dp[i] with previous dp to find optimal solution. Sometimes there 2 states for dp, like \"Best Time to Buy Stock\" problem:\n```\ndp[i][0]=max(dp[i-1][1]+price[i],dp[i-1][0])\ndp[i][1]=max(dp[i-1][0]-price[i],dp[i-1][1])\n```\n\n## Type 2 - Bottom up: Time Complexity O(N) Space Complexity O(N)\n\nTo find the optimal solution of ith item, we need to find the optimal item which fufill requirements in range [0,i]. It requires all the previous states stored. For example, in \"Perfect Square\",\n\n```\nfor i in range(n):\n    min_=sys.maxint\n    for j in range(i):\n        min_=min(dp[j],min_)\n    dp[i]=min_+1\n```\n\n## Type 3 - Top Down: Time Complexity O(N) Space Complexity O(N)\n\nTop-Down is memoiation. For example, \"Cheapest flights with K stop\":\n\n```\n    def bfs(n,flightlist,dst,s,k,mem):\n            \n            if k<-1:\n                return sys.maxint\n            \n            if s==dst:\n                return 0\n            else:\n                if mem[s][k]!=0:\n                    return mem[s][k]\n                \n                tmp=sys.maxint\n                for i in range(n):\n                    if flightlist[s][i]!=0:\n                        tmp=min(bfs(n,flightlist,dst,i,k-1,mem)+flightlist[s][i],tmp)\n                mem[s][k]=tmp\n                return tmp\n```\n\n## Bottom up\n\nNo | Question |State |State Relation\n---|---|---|---\n63 | Unique Path|number of unique path dp[i][j]|dp[i][j]=dp[i-1][j]+dp[i][j-1]\n64 | Minimum path sum | min path sum dp[i][j]|dp[i][j]=min(dp[i-1][j],dp[i][j-1])+grid[i][j]\n120| Triangle |min triangle path sum dp[i][j] |dp[i][j]=min(dp[i-1][j],dp[i-1][j-1])+triangle[i][j]\n152| Maximum product subarray | max product dp[i] | dp[i]=dp[i-1]*nums[i] (actually more complex due to positive and negative products)\n**198,213**|**House Robber I,II** | max amount of robbery dp[i] | dp[i]=max(dp[i-2],dp[i-3])+house[i] (Q II introduces circle, more condition to consider)\n**279**| **Perfect Squares** |least number of perfect squares dp[n]|Typical: for i in range(n):for j in range(i): dp[n]=min(dp[j])+1. Current optimal case comes from the optimal case within all the previous states. Space complexity is o(N)\n300| Longest Increasing Subsequence | longest increasing subsequence dp[n] | Simlilar to 279. for i in range(n):for j in range(i): if nums[i]>nums[j]:dp[i]=max(dp[j])+1\n**121** |**Best Time to buy and sell stock SERIES**|max value when holding and not holding stock after k transaction at nth day dp[n][k][1],dp[n][k][0] |dp[n][k][1]=max(dp[n-1][k][1],dp[n-1][k-1][0]-price[n]); dp[n][k][0]=max(dp[n-1][k][0],dp[n-1][k][1]+price[n]) only buy counted as one transaction in this assumption\n322| Coin change | min number of coin add up to amount n dp[n]|Similar to 279,300\n338| Counting Bits | number of 1's in binary form dp[n] | dp[n]=dp[n/2]+n%2 Found by sampling\n343| Integer Break | break integers to at least 2 integers and achieve max product dp[n] | Similar to 279,300\n211| Maximal square|max square from (0,0) to (i,j) dp[i][j] |dp[i][j]=min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+1\n**53**| **Maximual Subarray** |max subarray which must contains nums[n]: dp[n]| dp[n]=dp[n-1]+nums[n] if dp[n-1]>0 else dp[n]=nums[n] return max(dp)\n70| Climbing Stairs | distinct ways climbing to nth step dp[n]|dp[n]=dp[n-1]+dp[n-2]\n368| Largest Divisible Subset | largest divisible subset which must contains nums[n]: dp[n] |similar to 53\n377| Combination Sum IV | possible combinations dp[n]|dp[n]+=dp[n-item] for item in nums\n**416** | **Partition Equal Subset Sum (0/1 knapsack problem)**| specific sum j could be obtained from first i numbers: dp[i][j]| dp[i][j]=dp[i-1][j] or dp[i-1][j-nums[i]]\n**413** |**Arithmetic Slices** | number of slices end at nums[i]: dp[i]|if nums[i]-nums[i-1]==nums[i-1]-nums[i-2]: dp[i]=dp[i-1]+1\n**516**|**Longest Palindromic Subsequence**|longest palidromic subsequence length from i to j: dp[i][j]| if s[i]==s[j]:dp[i][j]=2+dp[i+1][j-1] if j-1>i+1 else 2, else dp[i][j]=max(dp[i+1)[j],dp[i][j-1]\n650|Two Key keyboard|min step needed to form n 'A': dp[n]| for i in range(n): for j in reversed(range(i-1)): if i%j==0:dp[i]=dp[j]+i/j\n673|Number of Longest Increasing Subsequence|longest increasing subsequence end with nums[n]|Upgrade version of 300 \n**718**|**Maximum Length of Repeated Subarray**|length of longest substring that end with A[i] and B[j]:dp[i][j]|if A[i]==B[j]:dp[i][j]=dp[i-1][j-1]+1\n712|Minimum ASCII Delete Sum for Two Strings|min sum of delete strings end with s1[i],s2[j]:dp[i][j]|similar to 718\n746|Min Cost Climbing Stairs|min cost which must contains cost[n]:dp[n]|dp[n]=min(dp[n-1],dp[n-2])+cost[n]\n**790**|**Domino and T Domino Tiling**| number of ways to form rectangle with lenght n:dp[n]|dp[n]=2*dp[n-1]+dp[n-3] (Quite hard to find)\n813|Largest Sum of Average| Largest sum in first i elements with k partitions: dp[i][k]|for k in range(K):for j in range(i):dp[i][k]=max(dp[i][k],sum(List[j:i])/(i-j)\n873|Length of longest Fibonacci Sequence| lenght of longest Fibonacci sequence from List[j] to List[i]: dp[j][i]|for i in range(len(List):for j in range(i):if List[i]-List[j] exist:dp[j][i]=dp[List[i]-List[j]][i]+1 ,result=max(result,max[j][i] \n\n\n\n## Top down\n\nNo|Question|Memory|Transition Function\n---|---|---|---\n139|Word Break| Substrings which cannot be broken into words |for word in words: if s[start:start+len(word)]==word: if recur():return true\n494|Target Sum|number of ways that S==sum(i numbers): mem[i][S]|count(i+1,s+nums[i])+count(i+1,s-nums[i])\n576|Out of Boundary Paths|number of paths at (i,j) and N steps left| path[i][j][N]=findpath(i,j+1,N-1)+findpath(i+1,j,N-1)+findpath(i-1,j)+findpath(i,j-1)\n646 |Maximum Length of Pair Chain|longest chain end with nums[n]:dp[n]|similar to 279\n688|Knight Probability in Chessboard|number of path that knight will go out of chessboard when k steps left|for i in range(8):count+=count(k-1,row+pair[i][0],column+pair[i][1])\n**787**|**Cheapest Flights with K Stops**| Min cost path which must contain flight i at k stop: mem[i][k]|for i in range(n):mem[i][k]=min(mem[s][k-1]+flight[i][k])\n\n\n## Special Case\n\nNo|Question | State | State Relation\n---|---|---|---\n264| Ugly Number II | nth ugly number dp[n] |Update mutiplier of 2,3,5 independently to find min ugly number dp[n] which is just larger than dp[n-1]\n304| Range Sum Query 2D -Immutable | sum of elements from (0,0) to (n,n) Sum[n][n] | Area = Sum[i2][j2]-Sum[i2][j1]-Sum[i1][j2]+Sum[i1][j1]. Immutable means the function would be callled many times, we need initiate states in __init__function.\n376| Wiggle Subsequence ||Greedy algorithm: only need to find number of max and min peak in sequence\n392| Is Subsequence |index of s | if s[index]==t[i]: index+=1 When index>len(s), the whole substring is found\n467| Unique Substrings in Wraparound String |Keep tracking longest string end with 'a-z': dict{'a-z':count} |Sum+=count(a-z), upgrade version of 413. The aggregation of number of substring follows the same rule: count(x)=length of longest string end with x \n647|Palindromic substrings| **Manacher's Algorithm**/Expand from center\n801|Minimun Swaps To Make Increasing Sequence|Wether swap A[i] and B[i] only depends on A[i-1] and B[i-1], so two dp variable should be updated. Best condition when A[i] and B[i] swap and best condition when A[i] and B[i] not swap.\n838|Pushing Dominos|final result and status of previous and current dominoes\n","tags":["Leetcode"]},{"title":"Face Swap Summary","url":"/2018/09/30/faceswap/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n\n# Face Swap Method 1\n\nFace swapping with Python, dlib, OpenCV \n\nhttps://github.com/matthewearl/faceswap\n\nFace swapping with Python dlib, Skimage(much easier in alignment)\n\nhttps://github.com/marsbroshok/face-replace\n\nFace Swapping with C++, dlib, histogram\n\n## Step\n1. Detecting facial landmarks\n2. Rotating, scaling, translating the second image to fit over the first (cv::getAffineTransoform or using SVD)\n3. Adjusting the color balance in the second image to match that of the first(another uses histogram matching)\n4. Blending feature from the second image on top of the first (A mask is used to select which parts of image2 and which parts of image1 should be shown in the final image)\n\n# Face Swap Method 2\n\nFace replace with Kinect, OpenCV, OpenGL\n\nhttps://github.com/jorticus/face-replace\n\n## Step\n1. Capture RGB+Depth video frame by Kinect\n2. Detect head pose and face features using Kinect SDK\n3. Deform the Candide-3 mesh to the given head pose and face features\n4. Process the RGB+Depth frames using OpenCV\n5. Draw the RGB video frame\n6. Draw the texture-mapped candide-3 model in OpenGL using a custom blend shader\n\n\n# Face Swap Method 3\n\nFace swapping with Python, OpenCV, dlib, pygame, PyOpenGL\n\nhttps://github.com/MarekKowalski/FaceSwap\n\n1. Detect face and landmarks\n2. Fit 3D model to landmarks\n3. Render 3D model using pygame with texture obtained during initialization\n4. Image of rendered model is blended with image obtained from camera using feathering(alpha blending) and simple color correction\n\n**Solved** Problems:\n1. Facial geometry of people varies quite a bit\n2. The lighting on the face combined with the tone of the skin can make the image look very different\n\n**Unsolved** Problems:\n1. The pose of the face can vary significantly\n2. The texture of the skin can vary from smooth to almost leathery\n\n# Face Swap Method 4 (similar to Method 1)\n\nOpenCV own lib\n\nhttps://github.com/spmallick/learnopencv/tree/master/FaceSwap\n\n**https://www.learnopencv.com/face-swap-using-opencv-c-python**\n\nhttps://github.com/t0nyren/piecewiseAffine\nhttps://github.com/trishume/faceHack\n\n1. Facial landmarks detection\n2. find convex hull\n3. delaunay triangulation\n4. affine wap triangles\n5. seamless cloning (自动合成)\n\n# Face Swap Method 5\n\nhttps://github.com/YuvalNirkin/face_swap\n\n\"On Face Segmentation, Face Swapping, and Face Perception\"\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537439562/blog/3.%20Face%20Swap/FaceSwap1.png)\n\n1. Fitting 3D face shapes\n2. Deep face segmentation\n3. Face Swapping and blending\n\n# Face Swap Method 6\n\n\"Transfiguring Portrait SIGGRAPH 2016\"\nhttps://homes.cs.washington.edu/~kemelmi/Transfiguring_Portraits_Kemelmacher_SIGGRAPH2016.pdf\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537948435/blog/3.%20Face%20Swap/FaceSwap2.png)\n\n## Face Processing\n1. Dectect Face and face landmarks\n2. Align and warp face to 3D model, estimate 3D pose\n3. Estimate features per photo: age, gender, HoG (histogram of gradient), face recognition features by VGG. \n\n## Similar set estimation\n\nPhotos of very similar face shapes are more amenable to effective composition. Target images are searched from online and typically include 1000 photos.\n\n1. Rank target photos based on their similarity to input photo. The similarity is computed as L2 norm between VGG face recognition features.\n2. Rerank selected photos in stage 1 by pose, age, HoG features:\n\n$$\nD(s,t)=||P_{s}-P_{t}||^2+||Age_{s}-Age_{t}||^2+X^2(H_{s},H_{t})\n$$\n\n## Systhesis\n\n1. Using Skin and hair algorithm to estimate mask\n2. Calculate an edge map from each mask and compute L2 norm between two masks. Pairs with smaller difference are ranked higher\n3. Align two photos using landm points, blend two photos given two mask by (Levin, 2004 Seamless image stitching in the gradient domain). \n\n\n# Real-time swapping \n\nDeepfake\n\nhttps://github.com/shaoanlu/faceswap-GAN\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1538318380/blog/3.%20Face%20Swap/deepfake.jpg)","tags":["Summary"]},{"title":"Action Recognition Model and Dataset Summary","url":"/2018/09/18/ActionRecognition/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n# Model 1: 3D-CNN\n\n[Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? 2017](https://arxiv.org/abs/1711.09577)\n## Introduction\nThe purpose of this study is to determine whether current video datasets have sufficient data for training very deep CNNs with spatio-temporal 3D kernel.\n\nUCF-101 and HMDB-51 provide videos with size around 10k, ActivityNet has 28k video clips, which are too small to be used for optimizing CNN representation from scratch. Kinetics Dataset has more than 300k video clips (Kinetics 600 has 500k clips now).\n\nThe 3D CNN architectures tested are based on ResNets and their 3D extented versions. The results show that Kinectics dataset can train 3D ResNet-152 from scratch. This is the **first work** to focus on the training of very deep 3D CNNs from scratch for action recognition.\n\n## Experiment Configuration\n\nResNet 18-200, ResNeXt and DenseNet are tested with Kinetics dataset in the study. ResNet 18,34, use basic block (Type A); ResNet 50,101,152,200 use bottleneck block (Type B); Pre-activation ResNet 200 was evaluated as well.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN3.png)\nThose deep CNNs are trained from scratch by Kinetics dataset and fined tune by UCF101, HMDB 51 and ActivityNet to evaluate model's transfer ability.\n\n### Implementation\n\n**Training**:\n\nDataPreprocessing:\n1. A 16-frame clip is generated around selected temporal position\n2. Randomly select a spatial position from the 4 corners and center\n3. Randomly select a spatial scale of sample to perform multi-scale cropping\n4. Spatially resize the sample at 112x112 pixels. The size of each sample is 3x16x112x112\n5. Each sample is horizontally flipped with 50% probability\n6. Subtract the mean values of ActivityNet from the sample for each color channel\n\nHyperparameters：\n1. cross-entropy loss and BP\n2. SGD with weight decay of 0.001 and 0.9 for momentum\n3. When training from scratch, lr starts from 0.1 and divide it by 10 after the validation loss saturates.\n4. When fine tuning, lr starts from 0.001 and has weight decay of 1e-5\n\n**Testing**:\n\n1. Each video is split into non-overlapped 16-frame clips.\n2. Each clip is spatially cropped around center position at scale 1\n3. Average output scores over all the clips of input video\n\n**All video clips in datasets are resized to heights of 240 pixels without changing their aspect ratios.**\n\n## Results and Discussion\n\n### Analysis of training from scratch\n\nTraining from scratch, ResNet-18 overfit both UCF-101, HMDB-51 and ActivityNet. The validation accuracies are 40.1, 16.2 and 26.8% respectively.\n\nKinetics could be used to train ResNet-18 without overfitting. As model depth increased, accuracies improved until reaching the depth of 152.\n\nResNeXt-101 achieved the best accuracies among the architectures testet.\n\nInput of 3x64x112x112 improves the accuracy by 4% on Kinetics dataset. Increase of resolution may also increase the accuracy like 3x64x224x224 in I3D.\n\n### Analysis of fine tuning\n\nIn this section, UCF-101 and HMDB-51 are used to fine tune Kinectics pretrained CNNs. Only conv5_x and FC layers are fine tuned because it achieved the best performance during the preliminary experiments.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN1.png)\n\nBased on results on UCF-101 and HMDB-51, simple 3D architectures pretrained on Kinetics outperform complex 2D architectures.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537163694/blog/4.%20Action%20Recognition/3D-CNN2.png)\n\n## Conclusion\n\n1. ResNet-18 training resulted in  significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics.\n2. Kinetics dataset has sufficient data for training deep 3D CNNs and enabling training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet.\n3. Kinetics pretrained simple 3D architectures outperforms complex 2D architectures on UCF-101 and HMDB-51, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51.\n\n\n# Model 2: Two-Stream I3D\n\n[Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset 2017](https://arxiv.org/abs/1705.07750)\n\nTwo-Stream Inflated 3D ConvNets(I3D) builds upon Inception V1, but inflates their filters and pooling kernels into 3D, leading to very deep, naturally spatio-temporal classifier.\n\n## Architecture\n\n### Inflating 2D ConvNets into 3D\n\nSimply inflating all the filters and pooling kernels in 2D architecture - endowing them with an additional temporal dimension. Filters are typically square and we just make them cubic - NxN to NxNxN\n\n### Bootstrapping 3D filters from 2D filters\n\nTo bootstrap parameters from the pretrained ImageNet models, it repeats the weights of 2D filters N times along the time dimension and rescaling them by dividing by N. \n\n### Pacing receptive field growth in space, time and network depth\n\nA symmetric receptive field is not necessarily optimal when considering time, it should depend on frame rate and image dimensions. If it grows too quickly in time relative to space, it may conflate edges from different objects breaking early feature detection, while if it grows too slowly, it may not capture scene dynamics well.\n\n64-frame snippets are used in training, the whole video is used and predictions are averaged in testing.\n\nThe overall architecture is shown below:\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537175423/blog/4.%20Action%20Recognition/I3D1.png)\n\n### Two 3D Streams\n\nIn experiments, it is found that two-stream configuration-with one I3D network trained on RGB inputs, and another on flow inputs which carry optimized, smooth flow information is valuable. These 2 networks are **trained separately** and predictions are averaged in test time.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537175947/blog/4.%20Action%20Recognition/I3D2.png)\n\n\n## Implementation\n**Traning:**\n\nData Preprocessing:\n1. video is sampled to 25 frames per second \n2. Resizing the smaller video side to 256 pixels, then randomly cropping 224x224 patch\n3. For shorter videos, we looped the video as many times as necessary to satisfy model's input interface\n4. Random left-right flipping applied\n\nHyperparameter:\n\n1. For all architectures, each convolutional layer follow by BN layer and ReLU activation function, except for the last convolutional layer which produce the class scores for each network.\n2. Standard SGD with momentun set to 0.9.\n3. 3D ConvNets utilize 64 GPUs with synchronous parallelization.\n4. Trained model on Kinetics fro 110k steps with 10x reduction of learning rate when val loss saturated.\n5. Implemented in Tensorflow\n\n**Test:**\nThe models are applied convolutionally over the whole video taking 224x224 center crops.\n\n**Optical Flow is computed with TV-L1 algorithm.**\n\n## Experiment and Discussion\n\n1. I3D outperformed all previous model, with either RGB, flow, or RGB+flow modalities.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537178242/blog/4.%20Action%20Recognition/I3D3.png)\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537178242/blog/4.%20Action%20Recognition/I3D4.png)\n2. ImageNet pre-training can extend to 3D ConvNets, improves accuracy.\n3. It seems plausible that RGB stream has more discriminative information - we often struggled with eyes to discern actions from flow alone in Kinetics. **There may be opportunities fro future research on integrating some form of motion stabilization into these architectures.**\n\n4. The study demonstrates transfer learning from one dataset(Kinetics) to anohter dataset(UCF-101/HMDB-51) for similar task. **However, it still remains to be seen if there is a benefit in using Kinetics pre-training for other video task such as semantic video segmentation, video object detection, or optical flow computation.**\n5. Action tubes or attention mechanism could be employed in the future.\n\n# Model 3: CNN+LSTM\n\n[Long-term Recurrent Convolutional Networks for Visual Recognition and Description 2014](https://arxiv.org/abs/1411.4389)\n\n## Introduction\n\nThis study instantiates proposed architecture in 3 experimental settings.\n1. Directly connect CNN model to deep LSTM network, to train video recognition models, which improves on the order of 4% on conventional benchmark\n2. Use CNN to encdoe a state vector, an LSTM to decode the vector into an natural language string, to train an end-to-end image to sentence mapping.\n3. Use conventional computer vision method to predict higher-level discriminative labels, enables LSTM decoders to generate language string\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537262401/blog/4.%20Action%20Recognition/LSTM_modified.jpg)\n\n## Architecture\n\nThe LRCN model works by passing each visual input $v_{t}$ (an image in isolation, or a frame from a video) through a feature transformation $\\phi_{V}(v_{t})$ parametrized by $V$ to produce a fixed-length vector representation $\\phi_{t}\\in R^d$. Having computed the feature space representation of the visual input sequence $<\\phi_{1},\\phi_{2},...,\\phi_{T}>$, the sequence model then takes over.\n\nThe final step in predicting a distribution $P(y_{t})$ is to take a softmax over the output $z_{t}$ of the sequence model.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM2.png)\nLRCN models is trained using SGD with momentum, with backpropagation to compute the gradient $\\nabla L(V,W)$ of the objective $L$ with respect to all parameters $(V,W)$. $W$ is parameter of LSTM, $V$ is parameter of CNN.\n\n## Action Recognition\n\nT individual frames are inputs into T convolutional networks which are then connected to a single-layer LSTM with 2565 hidden units. The study analyzes clips of 16 frames.\n\nIt explores 2 varaints of LRCN: LSTM is placed after CNN fc6 and CNN fc7. \n\n### Implementation\nIn training, video clip of 16 frames is fed into model. LRCN predicts the video class at each time step and then it averages these predicitons for final classification\n\nIn test, they extract 16 frame clips with a stride of 8 frames from each video and average across clips.\n\nBoth RGB and optical flow are used as input.\n\nCNN base of the LRCN is a hybrid of the Caffe reference model, a mior variant of AlexNet. The net is pre-trained on the 1.2 M image ILSVRC-2012  classification training subset of the ImageNet dataset.\n\n### Result\n\nRGB and flow networks can be combined by computing a weighted averagew fo network scores, which leads to better accuracy.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1537247425/blog/4.%20Action%20Recognition/LSTM3.png)\n\n# Dataset\n\n## Kinetics-600\n\nhttps://deepmind.com/research/open-source/open-source-datasets/kinetics/\n\n[The Kinetics Human Action Video Dataset](https://arxiv.org/abs/1705.06950)\n\nSize: around 500k\n\nClass: 600\n\nYear: 2018\n\nSource: \n1. The clips are from YouTube video, last 10s, and have a variable resolution and frame rate\n2. For an action class, all clips are from different YouTube videos\n\nNote: There is a standard test set (label released), and also a held-out test set(label not released, used for AcitivityNet Challenge). It encourage researchers to report results on the standard test set. \n## UCF-101\nhttp://crcv.ucf.edu/data/UCF101.php\n\n[UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild](http://crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf)\n\n\nSize: 13k\n\nLength: 27h\n\nClass: 101\n\nYear: 2011\n\nSource: realistc user-uploaded videos containing camera motion and cluttered background\n\n## HMDB-51\n\nhttp://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\n\n[HMDB: A Large Video Database for Human Motion Recognition. ICCV, 2011](http://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/Kuehne_etal_iccv11.pdf)\n\nSize: 7000 \n\nClass: 51\n\nYear: 2011\n\nSource: Range from digitized movies to YouTube\n\n\n\n\n\n\n\n\n\n","tags":["Action Recognition"]},{"title":"Summary from 23 Jul-10 Sep","url":"/2018/09/11/Summary-August-2018(1)/","content":"![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536636693/blog/2.%20Summary%20of%20August%202018/merge_from_ofoct.jpg)","tags":["Essay"]},{"title":"Spatial Transformer Network - Notes","url":"/2018/09/06/Notes/","content":"<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\ntex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n**In many computer vision tasks, distortion and transformation of image is highly needed. For example, in Cloth Visual Try on, which is the area i am interested in, cloth has to be distorted before putting on real-world human. Spatial Transfromer Network has shown impressive results in terms of image transformation and attention. It is worth reading this paper slowly and carefully.**\n\n***\n# Abstract\nSpatial Transformer Network explicitly allows the spatial manipulation of data within the network.\n\nit can be inserted into existing cnn to spatially transform feature maps and no extra tranining supervision or modification needed.\n\n# Introduction\nAction of spatial transformer is conditioned on individual data samples, with behaviour learnt during training.\n\nNot only select regions of an image that are most relevant, but aslo to transform those regions to canonical, expected pose.\n\nIt is trained with standard BP\n\n**application**\n1. image classification\n2. co-localisation\n3. **spatial attention**\n\na generalisation of differentiable attention to any transformation\n\n# Spatial Transformer Network\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/Spatial_transformer_network.png)\n\n3 parts:\n\n- **localisation network**: take input feature map and outputs parameters of the spatial transformation that should be applied to feature map--- gives transformation conditional on the input\n- **grid generator**: parameters form part 1 are used to create a sampling grid, wihch is a set of points where the input map should be sampled to produce the transformed output\n- **sampler**: feature map and sampling grid as input to sampler, producing output map sampled from the input at the grid points\n\n## Localisation Network\n\n\n$$\n\\theta=f_{loc}(U)\n$$\n\n$\\theta$ is output, $U$ is input, $f_{loc}$ is network function, it can take any form including FC network, CNN, but should include a final regression layer to produce transformation parameters $\\theta$\n\n## Parameterised Sampling Grid\nTo perform warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at particular location in the input feature map.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/sampling_grid.png)\nAssume $T_{\\theta}$ is a 2D affine transformation $A_{\\theta}$.\n\n$$\n\\left(\\begin{array}{cc}x_{i}^s \\\\ y_{i}^s\\end{array}\\right)=A_{\\theta}\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)=\\left[\\begin{array}{cc}\\theta_{11} &\\theta_{12} &\\theta_{13} \\\\ \\theta_{21} &\\theta_{22} &\\theta_{23}\\end{array}\\right]\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)\n$$\n\n$(x_{i}^t,y_{i}^t)$ are the target coordinates of the regular grid in the output feature map.\n\n$(x_{i}^s,y_{i}^s)$ are the source coordinates in the input feature map that define the sample points\n\n$A_{\\theta}$ is the affine transformation matrix\n\nOf course, $ T_{\\theta}$ or $A_{\\theta}$ can have any parameterised form, provided that it is deffirentiable with respect to the parameters---for BP\n\n**Thin plate spline transformation(TPS) is the most powerful transformation in experiments.**\n\n## Differentiable Image Sampling\n\nTo perform a spatial transformation of the input feature map, a sampler must take the set of sampling points $T_{\\theta}(G)$, along the the  input feature map $U$ and produce the smapled output feature map $V$\n\nEach $(x_{i}^s,y_{i}^d)$ coordinate in $T_{\\theta}(G)$ defines the spatial location in the input where a sampling kernel(such as bilinear) is applied to get the value at the particular pixel in the output $V$\n\n# Experiment\nExperiments are conducted on Distorted MNIST, Street View House Number for number recognition and CUB-200-2011 birds dataset by automatically discovering object parts and learning to attend to them.\n\n## Distorted MNIST\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/MNIST.png)\n\nSpatial transformer network acts on the input before the classification network, which are FCN and CNN, all STN use bilinear sampling, but use different transformation functions: affine, projective, 16 point thin plate spline (TPS)\n\n# Street View House Number\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228784/blog/1.%20Spatial%20Transformer%20Network/street_view_number.png)\nSpatial transformer network acts on the input before baseline CNN, and define another extension where before each of the first 4 convolutional layer of the baseline CNN. Regression layers of spatial transformer are initialised to predict the identity transformation. Affine transformation and bilinear sampling kernel are used\n\n# Fine-Grained Classification\n\nST-CNN is able to discover and learn part detectores in a data-driven manner without any additional supervision\n\n# Conclusion\n\nAccuracy improved in many tasks using spatial transformer network, it would be useful for tasks requiring the disentangling of object reference frames.\n\n\n\n","tags":["Transformation"]},{"title":"Hello World","url":"/2018/09/06/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]