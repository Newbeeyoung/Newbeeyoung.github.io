[{"title":"Spatial Transformer Network - Notes","url":"/2018/09/06/Notes/","content":"<script type=\"text/javascript\" async\n  src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n</script>\n**In many computer vision tasks, distortion and transformation of image is highly needed. For example, in Cloth Visual Try on, which is the area i am interested in, cloth has to be distorted before putting on real-world human. Spatial Transfromer Network has shown impressive results in terms of image transformation and attention. It is worth reading this paper slowly and carefully.**\n\n***\n# Abstract\nSpatial Transformer Network explicitly allows the spatial manipulation of data within the network.\n\nit can be inserted into existing cnn to spatially transform feature maps and no extra tranining supervision or modification needed.\n\n# Introduction\nAction of spatial transformer is conditioned on individual data samples, with behaviour learnt during training.\n\nNot only select regions of an image that are most relevant, but aslo to transform those regions to canonical, expected pose.\n\nIt is trained with standard BP\n\n**application**\n1. image classification\n2. co-localisation\n3. **spatial attention**\n\na generalisation of differentiable attention to any transformation\n\n# Spatial Transformer Network\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/Spatial_transformer_network.png)\n\n3 parts:\n\n- **localisation network**: take input feature map and outputs parameters of the spatial transformation that should be applied to feature map--- gives transformation conditional on the input\n- **grid generator**: parameters form part 1 are used to create a sampling grid, wihch is a set of points where the input map should be sampled to produce the transformed output\n- **sampler**: feature map and sampling grid as input to sampler, producing output map sampled from the input at the grid points\n\n## Localisation Network\n\n\n`$$\n\\theta=f_{loc}(U)\n$$`\n\n`$\\theta$` is output, `$U$` is input, `$f_{loc}$` is network function, it can take any form including FC network, CNN, but should include a final regression layer to produce transformation parameters $\\theta$\n\n## Parameterised Sampling Grid\nTo perform warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at particular location in the input feature map.\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/sampling_grid.png)\nAssume `$T_{\\theta}$` is a 2D affine transformation `$A_{\\theta}$`.\n\n$$\n\\left(\\begin{array}{cc}x_{i}^s \\\\ y_{i}^s\\end{array}\\right)=A_{\\theta}\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)=\\left[\\begin{array}{cc}\\theta_{11} &\\theta_{12} &\\theta_{13} \\\\ \\theta_{21} &\\theta_{22} &\\theta_{23}\\end{array}\\right]\\left(\\begin{array}{cc}x_{i}^t\\\\y_{i}^t\\\\1\\end{array}\\right)\n$$\n\n`$(x_{i}^t,y_{i}^t)$` are the target coordinates of the regular grid in the output feature map.\n\n`$(x_{i}^s,y_{i}^s)$` are the source coordinates in the input feature map that define the sample points\n\n`$A_{\\theta}$` is the affine transformation matrix\n\nOf course, `$ T_{\\theta}$` or `$A_{\\theta}$` can have any parameterised form, provided that it is deffirentiable with respect to the parameters---for BP\n\n**Thin plate spline transformation(TPS) is the most powerful transformation in experiments.**\n\n## Differentiable Image Sampling\n\nTo perform a spatial transformation of the input feature map, a sampler must take the set of sampling points `$T_{\\theta}(G)$`, along the the  input feature map `$U$` and produce the smapled output feature map `$V$`\n\nEach `$(x_{i}^s,y_{i}^d)$` coordinate in `$T_{\\theta}(G)$` defines the spatial location in the input where a sampling kernel(such as bilinear) is applied to get the value at the particular pixel in the output `$V$`\n\n# Experiment\nExperiments are conducted on Distorted MNIST, Street View House Number for number recognition and CUB-200-2011 birds dataset by automatically discovering object parts and learning to attend to them.\n\n## Distorted MNIST\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/MNIST.png)\n\nSpatial transformer network acts on the input before the classification network, which are FCN and CNN, all STN use bilinear sampling, but use different transformation functions: affine, projective, 16 point thin plate spline (TPS)\n\n# Street View House Number\n\n![image](https://res.cloudinary.com/dtifkxumc/image/upload/v1536228784/blog/1.%20Spatial%20Transformer%20Network/street_view_number.png)\nSpatial transformer network acts on the input before baseline CNN, and define another extension where before each of the first 4 convolutional layer of the baseline CNN. Regression layers of spatial transformer are initialised to predict the identity transformation. Affine transformation and bilinear sampling kernel are used\n\n# Fine-Grained Classification\n\nST-CNN is able to discover and learn part detectores in a data-driven manner without any additional supervision\n\n# Conclusion\n\nAccuracy improved in many tasks using spatial transformer network, it would be useful for tasks requiring the disentangling of object reference frames.\n\n\n\n","tags":["-Deep Learning -Transformation"]},{"title":"Hello World","url":"/2018/09/06/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]