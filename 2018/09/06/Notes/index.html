<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="易宸宇的博客">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        Spatial Transformer Network - Notes - Zone Yi
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 表里不一，知行合一 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/img/avatar.JPG" />
        </div>
        <div class="name">
            <i>Yi Chenyu</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>HOME</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>TAGS</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>ARCHIVES</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>ABOUT</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>SEARCH</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spatial-Transformer-Network"><span class="toc-text">Spatial Transformer Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Localisation-Network"><span class="toc-text">Localisation Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Parameterised-Sampling-Grid"><span class="toc-text">Parameterised Sampling Grid</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Differentiable-Image-Sampling"><span class="toc-text">Differentiable Image Sampling</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Experiment"><span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Distorted-MNIST"><span class="toc-text">Distorted MNIST</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Street-View-House-Number"><span class="toc-text">Street View House Number</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Fine-Grained-Classification"><span class="toc-text">Fine-Grained Classification</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Conclusion"><span class="toc-text">Conclusion</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">search</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i> 表里不一，知行合一 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        Spatial Transformer Network - Notes
    </div>

    <div class="post-meta">
        <span class="attr">Post：<span>2018-09-06 18:19:28</span></span>
        
        <span class="attr">Tags：/
        
        <a class="tag" href="/tags/#-Deep Learning -Transformation" title="-Deep Learning -Transformation">-Deep Learning -Transformation</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">Visit：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <p><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script><br><strong>In many computer vision tasks, distortion and transformation of image is highly needed. For example, in Cloth Visual Try on, which is the area i am interested in, cloth has to be distorted before putting on real-world human. Spatial Transfromer Network has shown impressive results in terms of image transformation and attention. It is worth reading this paper slowly and carefully.</strong></p>
<hr>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Spatial Transformer Network explicitly allows the spatial manipulation of data within the network.</p>
<p>it can be inserted into existing cnn to spatially transform feature maps and no extra tranining supervision or modification needed.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Action of spatial transformer is conditioned on individual data samples, with behaviour learnt during training.</p>
<p>Not only select regions of an image that are most relevant, but aslo to transform those regions to canonical, expected pose.</p>
<p>It is trained with standard BP</p>
<p><strong>application</strong></p>
<ol>
<li>image classification</li>
<li>co-localisation</li>
<li><strong>spatial attention</strong></li>
</ol>
<p>a generalisation of differentiable attention to any transformation</p>
<h1 id="Spatial-Transformer-Network"><a href="#Spatial-Transformer-Network" class="headerlink" title="Spatial Transformer Network"></a>Spatial Transformer Network</h1><p><img src="https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/Spatial_transformer_network.png" alt="image"></p>
<p>3 parts:</p>
<ul>
<li><strong>localisation network</strong>: take input feature map and outputs parameters of the spatial transformation that should be applied to feature map—- gives transformation conditional on the input</li>
<li><strong>grid generator</strong>: parameters form part 1 are used to create a sampling grid, wihch is a set of points where the input map should be sampled to produce the transformed output</li>
<li><strong>sampler</strong>: feature map and sampling grid as input to sampler, producing output map sampled from the input at the grid points</li>
</ul>
<h2 id="Localisation-Network"><a href="#Localisation-Network" class="headerlink" title="Localisation Network"></a>Localisation Network</h2><script type="math/tex; mode=display">
\theta=f_{loc}(U)</script><p>$\theta$ is output, $U$ is input, $f_{loc}$ is network function, it can take any form including FC network, CNN, but should include a final regression layer to produce transformation parameters $\theta$</p>
<h2 id="Parameterised-Sampling-Grid"><a href="#Parameterised-Sampling-Grid" class="headerlink" title="Parameterised Sampling Grid"></a>Parameterised Sampling Grid</h2><p>To perform warping of the input feature map, each output pixel is computed by applying a sampling kernel centered at particular location in the input feature map.</p>
<p><img src="https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/sampling_grid.png" alt="image"><br>Assume $T_{\theta}$ is a 2D affine transformation $A_{\theta}$.</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cc}x_{i}^s \\ y_{i}^s\end{array}\right)=A_{\theta}\left(\begin{array}{cc}x_{i}^t\\y_{i}^t\\1\end{array}\right)=\left[\begin{array}{cc}\theta_{11} &\theta_{12} &\theta_{13} \\ \theta_{21} &\theta_{22} &\theta_{23}\end{array}\right]\left(\begin{array}{cc}x_{i}^t\\y_{i}^t\\1\end{array}\right)</script><p>$(x_{i}^t,y_{i}^t)$ are the target coordinates of the regular grid in the output feature map.</p>
<p>$(x_{i}^s,y_{i}^s)$ are the source coordinates in the input feature map that define the sample points</p>
<p>$A_{\theta}$ is the affine transformation matrix</p>
<p>Of course, $ T_{\theta}$ or $A_{\theta}$ can have any parameterised form, provided that it is deffirentiable with respect to the parameters—-for BP</p>
<p><strong>Thin plate spline transformation(TPS) is the most powerful transformation in experiments.</strong></p>
<h2 id="Differentiable-Image-Sampling"><a href="#Differentiable-Image-Sampling" class="headerlink" title="Differentiable Image Sampling"></a>Differentiable Image Sampling</h2><p>To perform a spatial transformation of the input feature map, a sampler must take the set of sampling points $T_{\theta}(G)$, along the the  input feature map $U$ and produce the smapled output feature map $V$</p>
<p>Each $(x_{i}^s,y_{i}^d)$ coordinate in $T_{\theta}(G)$ defines the spatial location in the input where a sampling kernel(such as bilinear) is applied to get the value at the particular pixel in the output $V$</p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>Experiments are conducted on Distorted MNIST, Street View House Number for number recognition and CUB-200-2011 birds dataset by automatically discovering object parts and learning to attend to them.</p>
<h2 id="Distorted-MNIST"><a href="#Distorted-MNIST" class="headerlink" title="Distorted MNIST"></a>Distorted MNIST</h2><p><img src="https://res.cloudinary.com/dtifkxumc/image/upload/v1536228783/blog/1.%20Spatial%20Transformer%20Network/MNIST.png" alt="image"></p>
<p>Spatial transformer network acts on the input before the classification network, which are FCN and CNN, all STN use bilinear sampling, but use different transformation functions: affine, projective, 16 point thin plate spline (TPS)</p>
<h1 id="Street-View-House-Number"><a href="#Street-View-House-Number" class="headerlink" title="Street View House Number"></a>Street View House Number</h1><p><img src="https://res.cloudinary.com/dtifkxumc/image/upload/v1536228784/blog/1.%20Spatial%20Transformer%20Network/street_view_number.png" alt="image"><br>Spatial transformer network acts on the input before baseline CNN, and define another extension where before each of the first 4 convolutional layer of the baseline CNN. Regression layers of spatial transformer are initialised to predict the identity transformation. Affine transformation and bilinear sampling kernel are used</p>
<h1 id="Fine-Grained-Classification"><a href="#Fine-Grained-Classification" class="headerlink" title="Fine-Grained Classification"></a>Fine-Grained Classification</h1><p>ST-CNN is able to discover and learn part detectores in a data-driven manner without any additional supervision</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Accuracy improved in many tasks using spatial transformer network, it would be useful for tasks requiring the disentangling of object reference frames.</p>

        
        <div id="comment-container">
        </div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

</html>
